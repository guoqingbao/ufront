{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c4de2f",
   "metadata": {},
   "source": [
    "## Instll dependencies for Torch-MLIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5864c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: iree_compiler==20240129.785 in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (20240129.785)\n",
      "Requirement already satisfied: iree_runtime==20240129.785 in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (20240129.785)\n",
      "Requirement already satisfied: numpy in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (from iree_compiler==20240129.785) (1.26.0rc1)\n",
      "Requirement already satisfied: PyYAML in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (from iree_compiler==20240129.785) (6.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install iree_compiler==20240129.785 iree_runtime==20240129.785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ac8f274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iree-compiler      20240129.785\n",
      "iree-runtime       20240129.785\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep iree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85e6ec03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.5\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b94a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/llvm/torch-mlir/releases/tag/snapshot-20240127.1096 download and install torch-mlir and corresponding torch-cpu\n",
    "# Please note: recent torch-mlir only support Python3.8 and Python 3.9, for other python support, you may download older version of torch-mlir\n",
    "# !pip install https://github.com/llvm/torch-mlir/releases/download/snapshot-20240127.1096/torch_mlir-20240127.1096-cp311-cp311-linux_x86_64.whl --no-dependencies\n",
    "# !pip install https://github.com/llvm/torch-mlir/releases/download/snapshot-20240127.1096/torch-2.3.0.dev20240122+cpu-cp311-cp311-linux_x86_64.whl --no-dependencies\n",
    "# !pip install https://download.pytorch.org/whl/cpu/torchvision-0.18.0%2Bcpu-cp311-cp311-linux_x86_64.whl --no-dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bdd99ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch              2.3.0.dev20240122+cpu\n",
      "torch-mlir         20240127.1096\n",
      "torchvision        0.18.0+cpu\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb7a1d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Older IREE version used for older torch-mlir\n",
    "# !pip install iree-compiler==20230815.614 \n",
    "# !pip install https://github.com/iree-org/iree/releases/download/candidate-20230816.615/iree_runtime-20230816.615-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-dependencies --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27785efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (1.34.112)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.112 in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (from boto3) (1.34.112)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (from boto3) (0.10.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.112->boto3) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.112->boto3) (2.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.112->boto3) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tqdm in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (4.66.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#dependencies for torch-vit and bert model\n",
    "!pip install boto3\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65e6488",
   "metadata": {},
   "source": [
    "# Torch-MLIR Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42fb8162-39d8-4167-9ffd-11b25bdcc161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import io\n",
    "import numpy as np\n",
    "import time\n",
    "import torch_mlir\n",
    "from torchvision.models import resnet18, resnet50, squeezenet1_1, regnet_x_32gf, maxvit_t, shufflenet_v2_x1_5, inception_v3, mobilenet_v3_small, efficientnet_v2_s, densenet121, convnext_small\n",
    "import torchvision.models as models\n",
    "from iree import runtime\n",
    "from typing import Optional\n",
    "from torch.utils._pytree import tree_map\n",
    "import iree.runtime as ireert\n",
    "import iree.compiler as ireec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc40ecf0-0f53-465b-9911-5c54c15b9b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IREEInvoker:\n",
    "    \"\"\"A wrapper around an IREE module that provides a Pythonic interface.\n",
    "    \n",
    "    Specifically, this adapts `module.forward(...)` and similar calls into\n",
    "    lower-level calls into the functions in the IREE module, and also converts\n",
    "    between the IREE and Torch types.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, iree_module):\n",
    "        self._iree_module = iree_module\n",
    "        self.device = iree_module._context.config.device\n",
    "\n",
    "    def __getattr__(self, function_name: str):\n",
    "        def invoke(*args):\n",
    "            def wrap(x):\n",
    "                if isinstance(x, torch.Tensor):\n",
    "                    return ireert.asdevicearray(self.device, x)\n",
    "                return x\n",
    "            def unwrap(x):\n",
    "                if isinstance(x, ireert.DeviceArray):\n",
    "                    return torch.from_numpy(np.asarray(x).copy())\n",
    "                return x\n",
    "            iree_args = tree_map(wrap, args)\n",
    "            result = self._iree_module[function_name](*iree_args)\n",
    "            return tree_map(unwrap, result)\n",
    "        return invoke\n",
    "    \n",
    "def _map_target_backend_to_driver(target_backend):\n",
    "    if target_backend == \"cuda\":\n",
    "        return \"cuda\"\n",
    "    if target_backend == \"vulkan\":\n",
    "        return \"vulkan\"\n",
    "    if target_backend in (\"llvm-cpu\", \"vmvx\"):\n",
    "        return \"local-sync\"\n",
    "    raise ValueError(f\"Unknown target backend: {target_backend}\")\n",
    "\n",
    "def load_vmfb(flatbuffer, backend=\"llvm-cpu\"):\n",
    "    \"\"\"Load an IREE Flatbuffer into an in-process runtime wrapper.\n",
    "    The wrapper accepts and returns `torch.Tensor` types.\n",
    "    \"\"\"\n",
    "    config = ireert.Config(driver_name=_map_target_backend_to_driver(backend))\n",
    "    ctx = ireert.SystemContext(config=config)\n",
    "    vm_module = ireert.VmModule.from_flatbuffer(ctx.instance, flatbuffer)\n",
    "    ctx.add_vm_module(vm_module)\n",
    "    return IREEInvoker(ctx.modules.module)\n",
    "\n",
    "def compile_to_vmfb(mlir_module, target_backend=\"llvm-cpu\", \n",
    "                    cuda_llvm_target_arch: Optional[str] = None):\n",
    "    \"\"\"Compile an MLIR module to an IREE Flatbuffer.\n",
    "    The module is expected to be in the format produced by `torch_mlir.compile`\n",
    "    with `OutputType.LINALG_ON_TENSORS`.\n",
    "    TODO: Expose more compiler options.\n",
    "    \"\"\"\n",
    "    extra_args = []\n",
    "    if cuda_llvm_target_arch is not None:\n",
    "        arch_flag = f\"--iree-hal-cuda-llvm-target-arch={cuda_llvm_target_arch}\"\n",
    "        extra_args.append(arch_flag)\n",
    "    bytecode_stream = io.BytesIO()\n",
    "    mlir_module.operation.write_bytecode(bytecode_stream)\n",
    "    bytecode = bytecode_stream.getvalue()\n",
    "    \n",
    "    return ireec.compile_str(bytecode,\n",
    "                             target_backends=[target_backend],\n",
    "                             input_type=ireec.InputType.TM_TENSOR,\n",
    "                             extra_args=extra_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f685916",
   "metadata": {},
   "source": [
    "#### Vision Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c236e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**********Processing model MobileNetV3\n",
      "MobileNetV3****Compilation Time: 4.780s\n",
      "Calculating forward latency:\n",
      "  MobileNetV3 - 2.759 ± 0.003 ms\n",
      "\n",
      "**********Processing model ShuffleNetV2\n",
      "ShuffleNetV2****Compilation Time: 2.539s\n",
      "Calculating forward latency:\n",
      "  ShuffleNetV2 - 1.965 ± 0.005 ms\n",
      "\n",
      "**********Processing model ResNet18\n",
      "ResNet18****Compilation Time: 1.489s\n",
      "Calculating forward latency:\n",
      "  ResNet18 - 6.158 ± 0.021 ms\n",
      "\n",
      "**********Processing model ResNet50\n",
      "ResNet50****Compilation Time: 3.369s\n",
      "Calculating forward latency:\n",
      "  ResNet50 - 12.000 ± 0.000 ms\n",
      "\n",
      "**********Processing model SqueezeNet\n",
      "SqueezeNet****Compilation Time: 1.544s\n",
      "Calculating forward latency:\n",
      "  SqueezeNet - 1.241 ± 0.003 ms\n",
      "\n",
      "**********Processing model DenseNet121\n",
      "Lowering Torch Backend IR -> Linalg-on-Tensors Backend IR failed with the following diagnostics:\n",
      "\n",
      "\n",
      "python exception: Failure while executing pass pipeline:\n",
      "error: callsite(callsite(callsite(callsite(callsite(callsite(\"aten::batch_norm\"(\"/root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages/to\n",
      "\n",
      "**********Processing model InceptionV3\n",
      "InceptionV3****Compilation Time: 5.385s\n",
      "Calculating forward latency:\n",
      "  InceptionV3 - 13.200 ± 0.000 ms\n",
      "\n",
      "**********Processing model ViT_B16\n",
      "Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:\n",
      "\n",
      "\n",
      "python exception: Failure while executing pass pipeline:\n",
      "error: unknown: unsupported by backend contract: module initializers\n",
      "note: unknown: see current operation: \"torch.initialize.global_slots\"(%20, %21, %22, %23,\n"
     ]
    }
   ],
   "source": [
    "from benchmark import benchmark_module\n",
    "import io\n",
    "batch_size = 1\n",
    "input_sample = np.random.uniform(low=0.0, high=1.0, size=(batch_size, 3, 224, 224)).astype(np.float32)\n",
    "input = torch.Tensor(input_sample)\n",
    "model_list = {\"MobileNetV3\":mobilenet_v3_small(pretrained=False), \"ShuffleNetV2\":shufflenet_v2_x1_5(pretrained=False),\n",
    "            \"ResNet18\":resnet18(pretrained=False), \"ResNet50\":resnet50(pretrained=False), \"SqueezeNet\":squeezenet1_1(pretrained=False),\n",
    "            \"DenseNet121\":densenet121(pretrained=False), \"InceptionV3\":inception_v3(pretrained=False), \"ViT_B16\":models.vision_transformer.vit_b_16(weights=False)}\n",
    "\n",
    "# to make torch-mlir capable of compiling InceptionV3, remove jit trace check for the InceptionV3's forward function:\n",
    "    # def forward(self, x: Tensor):\n",
    "    #     x = self._transform_input(x)\n",
    "    #     x, aux = self._forward(x)\n",
    "    #     return x\n",
    "\n",
    "for modelname, model in model_list.items():\n",
    "    print(\"\\r\\n**********Processing model \" + modelname)\n",
    "    try: \n",
    "        model.train(mode=False)\n",
    "        t1_start = time.perf_counter()\n",
    "        \n",
    "        ts_graph = torch.jit.script(model)\n",
    "        module_ir = torch_mlir.compile(ts_graph, input,\n",
    "                                            output_type=torch_mlir.OutputType.LINALG_ON_TENSORS)\n",
    "\n",
    "        binary = compile_to_vmfb(module_ir, target_backend=\"cuda\")\n",
    "\n",
    "        t2_stop = time.perf_counter()\n",
    "        module = runtime.load_vm_flatbuffer(binary, driver=\"cuda\")\n",
    "\n",
    "        print(modelname + \"****Compilation Time: {:.3f}s\".format(t2_stop - t1_start)) # print performance indicator\n",
    "\n",
    "        print(\"Calculating forward latency:\\n  \", end=\"\")\n",
    "        tms = []\n",
    "        for i in range(10):\n",
    "            ret = benchmark_module(module.vm_module, entry_function=\"forward\", inputs=[\"1x3x224x224xf32=1\"], device=\"cuda\")\n",
    "            tm = ret[0].time\n",
    "            tms.append(float(tm[0:-3]))\n",
    "        print(\"{} - {:.3f} ± {:.3f} ms\".format(modelname, np.mean(tms), np.std(tms)))\n",
    "    except Exception as e:\n",
    "        print(str(e)[:300]) #print error head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bddd172",
   "metadata": {},
   "source": [
    "#### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eda0266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Module 'BertIntermediate' has no attribute 'intermediate_act_fn' (This attribute exists on the Python module, but we failed to convert Python type: 'functools.partial' to a TorchScript type. Only tensors and (possibly nested) tuples of tensors, lists, or dictsare supported as inputs or outputs of traced functions, but instead got value of type partial.. Its type was inferred; try adding a type annotation for the attribute.):\n",
      "  File \"/root/ufront/torch_bert.py\", line 343\n",
      "    def forward(self, hidden_states):\n",
      "        hidden_states = self.dense(hidden_states)\n",
      "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "                        ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n",
      "        return hidden_states\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import iree.compiler as ireec\n",
    "from iree import runtime\n",
    "from torch_bert import BertModel, BertConfig\n",
    "import torch\n",
    "import torch_mlir\n",
    "import time\n",
    "modelname = \"Bert\"\n",
    "input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "config = BertConfig(vocab_size_or_config_json_file=16000, hidden_size=768,\n",
    "    num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "model = BertModel(config=config)\n",
    "model.eval()\n",
    "try:\n",
    "    ts_graph = torch.jit.script(model)\n",
    "    module_ir = torch_mlir.compile(ts_graph, [input_ids, token_type_ids, input_mask], use_tracing=True,\n",
    "                                        output_type=torch_mlir.OutputType.LINALG_ON_TENSORS)\n",
    "\n",
    "    binary = compile_to_vmfb(module_ir, target_backend=\"llvm-cpu\")\n",
    "    compiled_model = runtime.load_vm_flatbuffer(binary,backend=\"llvm-cpu\")\n",
    "            \n",
    "    print(\"Performing benchmark...\")\n",
    "\n",
    "    t1_stop = time.perf_counter()\n",
    "    print(\"**** Model {} - Total Time: {:.3f}s\".format(modelname, t1_stop - t1_start)) # print performance indicator\n",
    "\n",
    "    tms = []\n",
    "    for i in range(10):\n",
    "        ret = benchmark_module(module.vm_module, entry_function=\"forward\", inputs=[input_ids.numpy(), token_type_ids.numpy(), input_mask.numpy()], device=\"cuda\")\n",
    "        tm = ret[0].time\n",
    "        tms.append(float(tm[0:-3]))\n",
    "    print(\"{} - {:.3f} ± {:.3f} ms\".format(modelname, np.mean(tms), np.std(tms)))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c024aa10",
   "metadata": {},
   "source": [
    "#### RNN/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "179f4463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:\n",
      "\n",
      "\n",
      "python exception: Failure while executing pass pipeline:\n",
      "error: \"aten::mul\"(\"/root/ufront/torch_def.py\":171:18): unsupported by backend contract: tensor with unknown rank\n",
      "note: \"aten::mul\"(\"/root/ufront/torch_def.py\":171:18): see current operation: %59 = \"torch.tensor_static_info_cast\"(%58) : (!torch.vtensor<[8,128],f32>) -> !torch.vtensor\n",
      "note: \"aten::mul\"(\"/root/ufront/torch_def.py\":171:18): this is likely due to a missing transfer function in abstract_interp_lib_gen.py\n",
      "\n",
      "For Torch-MLIR developers, the error can be reproduced with:\n",
      "$ torch-mlir-opt -pass-pipeline='builtin.module(torchscript-module-to-torch-backend-pipeline{backend-legal-ops=aten.flatten.using_ints,aten.adaptive_avg_pool1d extra-library=})' /tmp/SimpleLSTM.mlir\n",
      "Add '-mlir-print-ir-after-all -mlir-disable-threading' to get the IR dump for debugging purpose.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch_def import *\n",
    "import torch_mlir\n",
    "import numpy as np\n",
    "modelname = \"LSTM\"\n",
    "batch_size = 8\n",
    "hidden_size = 128\n",
    "seq_size = 32\n",
    "input_size = 256\n",
    "try: \n",
    "    t1_start = time.perf_counter()\n",
    "\n",
    "    input = np.ones((batch_size, seq_size,hidden_size)).astype(np.float32)\n",
    "    h0 = np.zeros((batch_size, hidden_size), dtype=np.float32)\n",
    "    c0 = np.zeros((batch_size, hidden_size), dtype=np.float32)\n",
    "\n",
    "    input, h0, c0 = torch.Tensor(input), torch.Tensor(h0), torch.Tensor(c0)\n",
    "    model = SimpleLSTM(input_size = 10, hidden_size = hidden_size, seq_size=seq_size)\n",
    "\n",
    "    model.train(mode=False)\n",
    "    ts_graph = torch.jit.script(model)\n",
    "    module_ir = torch_mlir.compile(ts_graph, [input, h0, c0], use_tracing=True,\n",
    "                                        output_type=torch_mlir.OutputType.LINALG_ON_TENSORS)\n",
    "\n",
    "    binary = compile_to_vmfb(module_ir, target_backend=\"llvm-cpu\")\n",
    "    compiled_model = runtime.load_vm_flatbuffer(binary,backend=\"llvm-cpu\")\n",
    "    print(\"Performing benchmark...\")\n",
    "\n",
    "    t1_stop = time.perf_counter()\n",
    "    print(\"**** Model {} - Total Time: {:.3f}s\".format(modelname, t1_stop - t1_start)) # print performance indicator\n",
    "\n",
    "    tms = []\n",
    "    for i in range(10):\n",
    "        ret = benchmark_module(module.vm_module, entry_function=\"forward\", inputs=[input.numpy(), h0.numpy(), c0.numpy()], device=\"cuda\")\n",
    "        tm = ret[0].time\n",
    "        tms.append(float(tm[0:-3]))\n",
    "    print(\"{} - {:.3f} ± {:.3f} ms\".format(modelname, np.mean(tms), np.std(tms)))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd1686",
   "metadata": {},
   "source": [
    "# UFront Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12e877c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping ufront as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mProcessing ./ufront-0.1.1-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "Requirement already satisfied: onnx in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (from ufront==0.1.1) (1.16.1)\n",
      "Requirement already satisfied: tf2onnx in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (from ufront==0.1.1) (1.16.1)\n",
      "Collecting onnxsim==0.4.17 (from ufront==0.1.1)\n",
      "  Obtaining dependency information for onnxsim==0.4.17 from https://files.pythonhosted.org/packages/a2/e8/eade1b53b5949af186826eb7cff35713cf157cc9b8056880e08a8cd75c48/onnxsim-0.4.17-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached onnxsim-0.4.17-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting rich (from onnxsim==0.4.17->ufront==0.1.1)\n",
      "  Obtaining dependency information for rich from https://files.pythonhosted.org/packages/87/67/a37f6214d0e9fe57f6ae54b2956d550ca8365857f42a1ce0392bb21d9410/rich-13.7.1-py3-none-any.whl.metadata\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (from onnx->ufront==0.1.1) (1.26.0rc1)\n",
      "Collecting protobuf>=3.20.2 (from onnx->ufront==0.1.1)\n",
      "  Obtaining dependency information for protobuf>=3.20.2 from https://files.pythonhosted.org/packages/96/a2/dc4d601c8a5c85b8e3eadf158a7f66696f8129ea3342fb69da60e96b9534/protobuf-5.27.0-cp38-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Using cached protobuf-5.27.0-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (from tf2onnx->ufront==0.1.1) (2.31.0)\n",
      "Requirement already satisfied: six in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (from tf2onnx->ufront==0.1.1) (1.16.0)\n",
      "Collecting flatbuffers>=1.12 (from tf2onnx->ufront==0.1.1)\n",
      "  Obtaining dependency information for flatbuffers>=1.12 from https://files.pythonhosted.org/packages/41/f0/7e988a019bc54b2dbd0ad4182ef2d53488bb02e58694cd79d61369e85900/flatbuffers-24.3.25-py2.py3-none-any.whl.metadata\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting protobuf>=3.20.2 (from onnx->ufront==0.1.1)\n",
      "  Obtaining dependency information for protobuf>=3.20.2 from https://files.pythonhosted.org/packages/8d/14/619e24a4c70df2901e1f4dbc50a6291eb63a759172558df326347dce1f0d/protobuf-3.20.3-py2.py3-none-any.whl.metadata\n",
      "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (from requests->tf2onnx->ufront==0.1.1) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (from requests->tf2onnx->ufront==0.1.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (from requests->tf2onnx->ufront==0.1.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (from requests->tf2onnx->ufront==0.1.1) (2023.7.22)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->onnxsim==0.4.17->ufront==0.1.1)\n",
      "  Obtaining dependency information for markdown-it-py>=2.2.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages (from rich->onnxsim==0.4.17->ufront==0.1.1) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->onnxsim==0.4.17->ufront==0.1.1)\n",
      "  Obtaining dependency information for mdurl~=0.1 from https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl.metadata\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached onnxsim-0.4.17-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m13.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m15.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:05\u001b[0m\n",
      "\u001b[?25hUsing cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: flatbuffers, protobuf, mdurl, markdown-it-py, rich, onnxsim, ufront\n",
      "Successfully installed flatbuffers-24.3.25 markdown-it-py-3.0.0 mdurl-0.1.2 onnxsim-0.4.17 protobuf-3.20.3 rich-13.7.1 ufront-0.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall ufront -y\n",
    "!pip install /root/ufront/ufront-0.1.1-cp311-cp311-manylinux_2_28_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f876afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import time\n",
    "import torch\n",
    "from torchvision.models import resnet18, resnet50, squeezenet1_1, regnet_x_32gf, maxvit_t, shufflenet_v2_x1_5, inception_v3, mobilenet_v3_small, efficientnet_v2_s, densenet121, convnext_small\n",
    "import torchvision.models as models\n",
    "from ufront.pytorch.model import UFrontTorch\n",
    "import argparse\n",
    "import ctypes\n",
    "from iree.compiler import tools\n",
    "from iree import runtime\n",
    "import iree.runtime as ireert\n",
    "import iree.compiler as ireec\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import iree.runtime as ireert\n",
    "import iree.compiler as ireec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c90483",
   "metadata": {},
   "source": [
    "#### Vision models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f93d74b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIX_TOSA_IR_FOR_NEW_IREE = True # for compitable with IREE recent release, make it to False if you use older IREE (e.g., before 20230830)\n",
    "def fix_tosa_for_new_iree(tosa_ir):\n",
    "    tosa_ir = tosa_ir.split(\"\\n\")\n",
    "    for i in range(len(tosa_ir)):\n",
    "        ir = tosa_ir[i][:300]\n",
    "        if ir.find(\"tosa.mul\") > 0:\n",
    "            tosa_ir[i] = tosa_ir[i].replace(\"i32\",\"i8\") # shift in mul become i8 type\n",
    "        elif ir.find(\"axis = \") > 0 or (ir.find(\"tosa.concat\") > 0 and tosa_ir[i].find(\"axis = \") > 0):\n",
    "            tosa_ir[i] = tosa_ir[i].replace(\"i64\",\"i32\") # axis attribute become i32 type\n",
    "    return \"\\n\".join(tosa_ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53d2f3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/torch-mlir/lib/python3.11/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3****Ufront->TOSA Time: 0.232s, TOSA->Binary Time: 2.532s, Total Time: 2.764s\n",
      "MobileNetV3 - 1.070 ± 0.000 ms\n",
      "ShuffleNetV2****Ufront->TOSA Time: 0.300s, TOSA->Binary Time: 1.990s, Total Time: 2.289s\n",
      "ShuffleNetV2 - 2.070 ± 0.008 ms\n",
      "ResNet18****Ufront->TOSA Time: 0.339s, TOSA->Binary Time: 1.636s, Total Time: 1.975s\n",
      "ResNet18 - 2.731 ± 0.003 ms\n",
      "ResNet50****Ufront->TOSA Time: 0.763s, TOSA->Binary Time: 3.233s, Total Time: 3.996s\n",
      "ResNet50 - 6.151 ± 0.010 ms\n",
      "SqueezeNet****Ufront->TOSA Time: 0.064s, TOSA->Binary Time: 1.246s, Total Time: 1.310s\n",
      "SqueezeNet - 1.170 ± 0.000 ms\n",
      "DenseNet121****Ufront->TOSA Time: 1.121s, TOSA->Binary Time: 5.600s, Total Time: 6.720s\n",
      "DenseNet121 - 8.169 ± 0.018 ms\n",
      "InceptionV3****Ufront->TOSA Time: 0.789s, TOSA->Binary Time: 4.526s, Total Time: 5.315s\n",
      "InceptionV3 - 12.420 ± 0.040 ms\n",
      "ViT_B16****Ufront->TOSA Time: 2.344s, TOSA->Binary Time: 4.947s, Total Time: 7.291s\n",
      "ViT_B16 - 35.150 ± 0.092 ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from benchmark import benchmark_module\n",
    "batch_size = 1\n",
    "input_sample = np.random.uniform(low=0.0, high=1.0, size=(batch_size, 3, 224, 224)).astype(np.float32)\n",
    "input = torch.Tensor(input_sample)\n",
    "\n",
    "model_list = {\"MobileNetV3\":mobilenet_v3_small(pretrained=False), \"ShuffleNetV2\":shufflenet_v2_x1_5(pretrained=False),\n",
    "            \"ResNet18\":resnet18(pretrained=False), \"ResNet50\":resnet50(pretrained=False), \"SqueezeNet\":squeezenet1_1(pretrained=False),\n",
    "            \"DenseNet121\":densenet121(pretrained=False), \"InceptionV3\":inception_v3(pretrained=False), \"ViT_B16\":models.vision_transformer.vit_b_16(weights=False, dropout=0.1)}\n",
    "\n",
    "for modelname, net in model_list.items():\n",
    "    net.train(False) \n",
    "\n",
    "    t1_start = time.perf_counter()\n",
    "    model = UFrontTorch(net, batch_size=batch_size, pass_weights=True) # convert torch model to ufront model\n",
    "    #This will trigger Rust frontend for actual model conversion and graph building\n",
    "    #operators can also be managed by python side (each operator here corresponding to an operator in the Rust computation graph)\n",
    "    output_tensors = model(inputs = [input])\n",
    "\n",
    "    #This will trigger model compilation, i.e., convert Rust computation graph to a unified high-level IR and lower it to TOSA IR\n",
    "    model.compile(optimizer={\"type\":\"sgd\", \"lr\":\"0.01\", \"momentum\":\"0\", \"nesterov\":\"False\", \"weight_decay\":\"0\"},\n",
    "                        loss='sparse_categorical_crossentropy', metrics=['accuracy', 'sparse_categorical_crossentropy'])\n",
    "    tosa_ir= model.dump_tosa_ir()\n",
    "    t1_stop = time.perf_counter()\n",
    "\n",
    "    if FIX_TOSA_IR_FOR_NEW_IREE:\n",
    "        tosa_ir = fix_tosa_for_new_iree(tosa_ir)\n",
    "\n",
    "    binary = ireec.compile_str(tosa_ir,\n",
    "                    target_backends=[\"cuda\"], \n",
    "                    input_type=ireec.InputType.TOSA)\n",
    "    t2_stop = time.perf_counter()\n",
    "\n",
    "    print(modelname + \"****Ufront->TOSA Time: {:.3f}s, TOSA->Binary Time: {:.3f}s, Total Time: {:.3f}s\".format(t1_stop - t1_start, t2_stop - t1_stop, t2_stop - t1_start)) # print performance indicator\n",
    "    module = runtime.load_vm_flatbuffer(binary, driver=\"cuda\")\n",
    "    tms = []\n",
    "    for i in range(10):\n",
    "        ret = benchmark_module(module.vm_module, entry_function=\"forward\", inputs=[\"1x3x224x224xf32=1\"], device=\"cuda\")\n",
    "        tm = ret[0].time\n",
    "        tms.append(float(tm[0:-3]))\n",
    "    print(\"{} - {:.3f} ± {:.3f} ms\".format(modelname, np.mean(tms), np.std(tms)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684ffd50",
   "metadata": {},
   "source": [
    "#### Language Model (Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "327f24a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling TOSA model...\n",
      "Compiling Binary...\n",
      "Bert****Ufront->TOSA Time: 2.545s, TOSA->Binary Time: 5.312s, Total Time: 7.857s\n",
      "Bert - 5.716 ± 0.018 ms\n"
     ]
    }
   ],
   "source": [
    "from ufront.pytorch.model import UFrontTorch \n",
    "import iree.compiler as ireec\n",
    "from iree import runtime\n",
    "from benchmark import benchmark_module\n",
    "from torch_bert import BertModel, BertConfig\n",
    "import torch\n",
    "import time\n",
    "modelname = \"Bert\"\n",
    "input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "config = BertConfig(vocab_size_or_config_json_file=16000, hidden_size=768,\n",
    "    num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "net = BertModel(config=config)\n",
    "net.eval()\n",
    "\n",
    "t1_start = time.perf_counter()\n",
    "model = UFrontTorch(net, batch_size=1, pass_weights=True) # convert torch model to ufront model\n",
    "output_tensors = model(inputs = [input_ids, token_type_ids, input_mask])\n",
    "\n",
    "model.compile(optimizer={\"type\":\"sgd\", \"lr\":\"0.01\", \"momentum\":\"0\", \"nesterov\":\"False\", \"weight_decay\":\"0\"},\n",
    "                    loss='sparse_categorical_crossentropy', metrics=['accuracy', 'sparse_categorical_crossentropy'])\n",
    "\n",
    "print(\"Compiling TOSA model...\")\n",
    "tosa_ir= model.dump_tosa_ir()\n",
    "t1_stop = time.perf_counter()\n",
    "if FIX_TOSA_IR_FOR_NEW_IREE:\n",
    "    tosa_ir = fix_tosa_for_new_iree(tosa_ir)\n",
    "print(\"Compiling Binary...\")\n",
    "binary = ireec.compile_str(tosa_ir,\n",
    "                target_backends=[\"cuda\"], \n",
    "                input_type=ireec.InputType.TOSA)\n",
    "t2_stop = time.perf_counter()\n",
    "print(\"Bert****Ufront->TOSA Time: {:.3f}s, TOSA->Binary Time: {:.3f}s, Total Time: {:.3f}s\".format(t1_stop - t1_start, t2_stop - t1_stop, t2_stop - t1_start)) # print performance indicator\n",
    "module = runtime.load_vm_flatbuffer(binary, driver=\"cuda\")\n",
    "\n",
    "# %timeit -n 100 module.forward(input_ids, token_type_ids, input_mask)\n",
    "tms = []\n",
    "for i in range(10):\n",
    "    ret = benchmark_module(module.vm_module, entry_function=\"forward\", inputs=[input_ids.numpy(), token_type_ids.numpy(), input_mask.numpy()], device=\"cuda\")\n",
    "    tm = ret[0].time\n",
    "    tms.append(float(tm[0:-3]))\n",
    "print(\"{} - {:.3f} ± {:.3f} ms\".format(modelname, np.mean(tms), np.std(tms)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aff4f3",
   "metadata": {},
   "source": [
    "#### RNN/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59bf17f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM****Ufront->TOSA Time: 0.047s, TOSA->Binary Time: 1.097s, Total Time: 1.145s\n",
      "LSTM - 0.432 ± 0.006 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ufront.pytorch.model import UFrontTorch\n",
    "import iree.compiler as ireec\n",
    "from iree import runtime\n",
    "from benchmark import benchmark_module\n",
    "import time\n",
    "import torch\n",
    "from torch_def import *\n",
    "import numpy as np\n",
    "batch_size = 8\n",
    "hidden_size = 128\n",
    "seq_size = 32\n",
    "input_size = 256\n",
    "modelname = \"LSTM\"\n",
    "input = np.ones((batch_size, seq_size,hidden_size)).astype(np.float32)\n",
    "h0 = np.zeros((batch_size, hidden_size), dtype=np.float32)\n",
    "c0 = np.zeros((batch_size, hidden_size), dtype=np.float32)\n",
    "t1_start = time.perf_counter()\n",
    "input, h0, c0 = torch.Tensor(input), torch.Tensor(h0), torch.Tensor(c0)\n",
    "lstm = SimpleLSTM(input_size = 10, hidden_size = hidden_size, seq_size=seq_size)\n",
    "model = UFrontTorch(lstm, batch_size=batch_size, pass_weights=True)\n",
    "output_tensors = model(inputs = [input, h0, c0])\n",
    "model.compile(optimizer={\"type\":\"sgd\", \"lr\":\"0.01\", \"momentum\":\"0\", \"nesterov\":\"False\", \"weight_decay\":\"0\"},\n",
    "                      loss='sparse_categorical_crossentropy', metrics=['accuracy', 'sparse_categorical_crossentropy'])\n",
    "\n",
    "tosa_ir = model.dump_tosa_ir()\n",
    "t1_stop = time.perf_counter()\n",
    "\n",
    "if FIX_TOSA_IR_FOR_NEW_IREE:\n",
    "    tosa_ir = fix_tosa_for_new_iree(tosa_ir)\n",
    "\n",
    "binary = ireec.compile_str(tosa_ir,\n",
    "                target_backends=[\"cuda\"], \n",
    "                input_type=ireec.InputType.TOSA)\n",
    "t2_stop = time.perf_counter()\n",
    "\n",
    "print(\"LSTM****Ufront->TOSA Time: {:.3f}s, TOSA->Binary Time: {:.3f}s, Total Time: {:.3f}s\".format(t1_stop - t1_start, t2_stop - t1_stop, t2_stop - t1_start)) # print performance indicator\n",
    "\n",
    "module = runtime.load_vm_flatbuffer(binary, driver=\"cuda\")\n",
    "\n",
    "tms = []\n",
    "for i in range(10):\n",
    "    ret = benchmark_module(module.vm_module, entry_function=\"forward\", inputs=[input.numpy(), h0.numpy(), c0.numpy()], device=\"cuda\")\n",
    "    tm = ret[0].time\n",
    "    tms.append(float(tm[0:-3]))\n",
    "print(\"{} - {:.3f} ± {:.3f} ms\".format(modelname, np.mean(tms), np.std(tms)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
