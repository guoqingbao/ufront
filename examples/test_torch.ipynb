{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42502569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 22 15:09:49 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:21:00.0 Off |                  N/A |\n",
      "|  0%   29C    P8    16W / 240W |     46MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c387476e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4aefc1ef",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d0f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install iree-compiler==20230524.529 iree-runtime==20230524.529 -f https://github.com/iree-org/iree/releases/tag/candidate-20230512.517"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d46328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CUDA 11\n",
    "!pip install iree-compiler==20230524.529 iree-runtime==20230524.529 \n",
    "!pip install iree-tools-tf==20230524.529  iree-tools-tflite==20230524.529\n",
    "\n",
    "# For CUDA 12\n",
    "\n",
    "# !pip install iree-compiler==20230815.614 iree-runtime==20230815.614\n",
    "# !pip install iree-tools-tf==20230815.614  iree-tools-tflite==20230815.614\n",
    "# fix issue of iree-benchmark-module for iree-compiler (v20230815.614), depend on the installation of IREE package\n",
    "# ls /opt/conda/lib/python3.10/site-packages/iree/_runtime_libs/\n",
    "# cp /opt/conda/lib/python3.10/site-packages/iree/_runtime_libs/iree-benchmark-module /opt/conda/lib/python3.10/site-packages/iree/runtime/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac8f274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iree-compiler                      20230524.529\n",
      "iree-runtime                       20230524.529\n",
      "iree-tools-tf                      20230524.529\n",
      "iree-tools-tflite                  20230524.529\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep iree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a10c935",
   "metadata": {},
   "source": [
    "#### Install Torch-MLIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b94a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/llvm/torch-mlir/releases/tag/snapshot-20230525.849 download and install torch-mlir and corresponding torch-cpu\n",
    "# !pip install torch-mlir==20230525.849 -f https://github.com/llvm/torch-mlir/releases/download/snapshot-20230525.849/torch_mlir-20230525.849-cp310-cp310-linux_x86_64.whl --no-dependencies\n",
    "# !pip install torch==2.1.0.dev20230523 -f https://github.com/llvm/torch-mlir/releases/download/snapshot-20230525.849/torch-2.1.0.dev20230523+cpu-cp310-cp310-linux_x86_64.whl --no-dependencies\n",
    "# !pip install torchvision==0.16.0 --no-dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bdd99ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip list | grep torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a485a0",
   "metadata": {},
   "source": [
    "# Torch-MLIR Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42fb8162-39d8-4167-9ffd-11b25bdcc161",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/triton/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import io\n",
    "import numpy as np\n",
    "import time\n",
    "import torch_mlir\n",
    "from torchvision.models import resnet18, resnet50, squeezenet1_1, regnet_x_32gf, maxvit_t, shufflenet_v2_x1_5, inception_v3, mobilenet_v3_small, efficientnet_v2_s, densenet121, convnext_small\n",
    "import torchvision.models as models\n",
    "from iree import runtime\n",
    "from typing import Optional\n",
    "from torch.utils._pytree import tree_map\n",
    "import iree.runtime as ireert\n",
    "import iree.compiler as ireec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc40ecf0-0f53-465b-9911-5c54c15b9b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IREEInvoker:\n",
    "    \"\"\"A wrapper around an IREE module that provides a Pythonic interface.\n",
    "    \n",
    "    Specifically, this adapts `module.forward(...)` and similar calls into\n",
    "    lower-level calls into the functions in the IREE module, and also converts\n",
    "    between the IREE and Torch types.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, iree_module):\n",
    "        self._iree_module = iree_module\n",
    "        self.device = iree_module._context.config.device\n",
    "\n",
    "    def __getattr__(self, function_name: str):\n",
    "        def invoke(*args):\n",
    "            def wrap(x):\n",
    "                if isinstance(x, torch.Tensor):\n",
    "                    return ireert.asdevicearray(self.device, x)\n",
    "                return x\n",
    "            def unwrap(x):\n",
    "                if isinstance(x, ireert.DeviceArray):\n",
    "                    return torch.from_numpy(np.asarray(x).copy())\n",
    "                return x\n",
    "            iree_args = tree_map(wrap, args)\n",
    "            result = self._iree_module[function_name](*iree_args)\n",
    "            return tree_map(unwrap, result)\n",
    "        return invoke\n",
    "    \n",
    "def _map_target_backend_to_driver(target_backend):\n",
    "    if target_backend == \"cuda\":\n",
    "        return \"cuda\"\n",
    "    if target_backend == \"vulkan\":\n",
    "        return \"vulkan\"\n",
    "    if target_backend in (\"llvm-cpu\", \"vmvx\"):\n",
    "        return \"local-sync\"\n",
    "    raise ValueError(f\"Unknown target backend: {target_backend}\")\n",
    "\n",
    "def load_vmfb(flatbuffer, backend=\"llvm-cpu\"):\n",
    "    \"\"\"Load an IREE Flatbuffer into an in-process runtime wrapper.\n",
    "    The wrapper accepts and returns `torch.Tensor` types.\n",
    "    \"\"\"\n",
    "    config = ireert.Config(driver_name=_map_target_backend_to_driver(backend))\n",
    "    ctx = ireert.SystemContext(config=config)\n",
    "    vm_module = ireert.VmModule.from_flatbuffer(ctx.instance, flatbuffer)\n",
    "    ctx.add_vm_module(vm_module)\n",
    "    return IREEInvoker(ctx.modules.module)\n",
    "\n",
    "def compile_to_vmfb(mlir_module, target_backend=\"llvm-cpu\", \n",
    "                    cuda_llvm_target_arch: Optional[str] = None):\n",
    "    \"\"\"Compile an MLIR module to an IREE Flatbuffer.\n",
    "    The module is expected to be in the format produced by `torch_mlir.compile`\n",
    "    with `OutputType.LINALG_ON_TENSORS`.\n",
    "    TODO: Expose more compiler options.\n",
    "    \"\"\"\n",
    "    extra_args = []\n",
    "    if cuda_llvm_target_arch is not None:\n",
    "        arch_flag = f\"--iree-hal-cuda-llvm-target-arch={cuda_llvm_target_arch}\"\n",
    "        extra_args.append(arch_flag)\n",
    "    bytecode_stream = io.BytesIO()\n",
    "    mlir_module.operation.write_bytecode(bytecode_stream)\n",
    "    bytecode = bytecode_stream.getvalue()\n",
    "    \n",
    "    return ireec.compile_str(bytecode,\n",
    "                             target_backends=[target_backend],\n",
    "                             input_type=ireec.InputType.TM_TENSOR,\n",
    "                             extra_args=extra_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b96c9eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6f07091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b4072",
   "metadata": {},
   "source": [
    "#### Vision Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b4c0c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**********Processing model MobileNetV3\n",
      "MobileNetV3****Compilation Time: 3.508s\n",
      "Calculating forward latency:\n",
      "  MobileNetV3 - 1.628 ± 0.004 ms\n",
      "\n",
      "**********Processing model ShuffleNetV2\n",
      "Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:\n",
      "\n",
      "\n",
      "python exceptio\n",
      "\n",
      "**********Processing model ResNet18\n",
      "ResNet18****Compilation Time: 1.651s\n",
      "Calculating forward latency:\n",
      "  ResNet18 - 6.510 ± 0.021 ms\n",
      "\n",
      "**********Processing model ResNet50\n",
      "ResNet50****Compilation Time: 3.494s\n",
      "Calculating forward latency:\n",
      "  ResNet50 - 15.080 ± 0.040 ms\n",
      "\n",
      "**********Processing model SqueezeNet\n",
      "SqueezeNet****Compilation Time: 1.657s\n",
      "Calculating forward latency:\n",
      "  SqueezeNet - 1.650 ± 0.000 ms\n",
      "\n",
      "**********Processing model DenseNet121\n",
      "Lowering Torch Backend IR -> Linalg-on-Tensors Backend IR failed with the following diagnostics:\n",
      "\n",
      "\n",
      "p\n",
      "\n",
      "**********Processing model InceptionV3\n",
      "Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:\n",
      "\n",
      "\n",
      "python exceptio\n",
      "\n",
      "**********Processing model ViT_B16\n",
      "Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:\n",
      "\n",
      "\n",
      "python exceptio\n"
     ]
    }
   ],
   "source": [
    "from benchmark import benchmark_module\n",
    "batch_size = 1\n",
    "input_sample = np.random.uniform(low=0.0, high=1.0, size=(batch_size, 3, 224, 224)).astype(np.float32)\n",
    "input = torch.Tensor(input_sample)\n",
    "model_list = {\"MobileNetV3\":mobilenet_v3_small(pretrained=False), \"ShuffleNetV2\":shufflenet_v2_x1_5(pretrained=False),\n",
    "            \"ResNet18\":resnet18(pretrained=False), \"ResNet50\":resnet50(pretrained=False), \"SqueezeNet\":squeezenet1_1(pretrained=False),\n",
    "            \"DenseNet121\":densenet121(pretrained=False), \"InceptionV3\":inception_v3(pretrained=False), \"ViT_B16\":models.vision_transformer.vit_b_16(weights=False)}\n",
    "\n",
    "for modelname, model in model_list.items():\n",
    "    print(\"\\r\\n**********Processing model \" + modelname)\n",
    "    try: \n",
    "        model.train(mode=False)\n",
    "        t1_start = time.perf_counter()\n",
    "        \n",
    "        ts_graph = torch.jit.script(model)\n",
    "        module_ir = torch_mlir.compile(ts_graph, input,\n",
    "                                            output_type=torch_mlir.OutputType.LINALG_ON_TENSORS)\n",
    "\n",
    "        binary = compile_to_vmfb(module_ir, target_backend=\"cuda\")\n",
    "\n",
    "        t2_stop = time.perf_counter()\n",
    "        module = runtime.load_vm_flatbuffer(binary, driver=\"cuda\")\n",
    "\n",
    "        print(modelname + \"****Compilation Time: {:.3f}s\".format(t2_stop - t1_start)) # print performance indicator\n",
    "\n",
    "        print(\"Calculating forward latency:\\n  \", end=\"\")\n",
    "        tms = []\n",
    "        for i in range(10):\n",
    "            ret = benchmark_module(module.vm_module, entry_function=\"forward\", inputs=[\"1x3x224x224xf32=1\"], device=\"cuda\")\n",
    "            tm = ret[0].time\n",
    "            tms.append(float(tm[0:-3]))\n",
    "        print(\"{} - {:.3f} ± {:.3f} ms\".format(modelname, np.mean(tms), np.std(tms)))\n",
    "    except Exception as e:\n",
    "        print(str(e)[:100]) #print error head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee43b96",
   "metadata": {},
   "source": [
    "#### Language Model (Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eda0266",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\nModule 'BertIntermediate' has no attribute 'intermediate_act_fn' (This attribute exists on the Python module, but we failed to convert Python type: 'functools.partial' to a TorchScript type. Only tensors and (possibly nested) tuples of tensors, lists, or dictsare supported as inputs or outputs of traced functions, but instead got value of type partial.. Its type was inferred; try adding a type annotation for the attribute.):\n  File \"/root/ufront/torch_bert.py\", line 343\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n                        ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n        return hidden_states\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m BertModel(config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 17\u001b[0m ts_graph \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m module_ir \u001b[38;5;241m=\u001b[39m torch_mlir\u001b[38;5;241m.\u001b[39mcompile(ts_graph, input_ids,\n\u001b[1;32m     19\u001b[0m                                     output_type\u001b[38;5;241m=\u001b[39mtorch_mlir\u001b[38;5;241m.\u001b[39mOutputType\u001b[38;5;241m.\u001b[39mLINALG_ON_TENSORS)\n\u001b[1;32m     21\u001b[0m binary \u001b[38;5;241m=\u001b[39m compile_to_vmfb(module_ir, target_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllvm-cpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/triton/lib/python3.10/site-packages/torch/jit/_script.py:1284\u001b[0m, in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m   1283\u001b[0m     obj \u001b[38;5;241m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[0;32m-> 1284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_script_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_methods_to_compile\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m create_script_dict(obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/triton/lib/python3.10/site-packages/torch/jit/_recursive.py:480\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing:\n\u001b[1;32m    479\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[38;5;241m.\u001b[39mcheck(nn_module)\n\u001b[0;32m--> 480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/triton/lib/python3.10/site-packages/torch/jit/_recursive.py:542\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    539\u001b[0m     script_module\u001b[38;5;241m.\u001b[39m_concrete_type \u001b[38;5;241m=\u001b[39m concrete_type\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m script_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecursiveScriptModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpp_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m~/anaconda3/envs/triton/lib/python3.10/site-packages/torch/jit/_script.py:614\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;124;03mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m script_module \u001b[38;5;241m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> 614\u001b[0m \u001b[43minit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    618\u001b[0m RecursiveScriptModule\u001b[38;5;241m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m~/anaconda3/envs/triton/lib/python3.10/site-packages/torch/jit/_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    517\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m orig_value\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;66;03m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_concrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m cpp_module\u001b[38;5;241m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    523\u001b[0m script_module\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m scripted\n",
      "File \u001b[0;32m~/anaconda3/envs/triton/lib/python3.10/site-packages/torch/jit/_recursive.py:542\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    539\u001b[0m     script_module\u001b[38;5;241m.\u001b[39m_concrete_type \u001b[38;5;241m=\u001b[39m concrete_type\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m script_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecursiveScriptModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpp_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m~/anaconda3/envs/triton/lib/python3.10/site-packages/torch/jit/_script.py:614\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;124;03mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m script_module \u001b[38;5;241m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> 614\u001b[0m \u001b[43minit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    618\u001b[0m RecursiveScriptModule\u001b[38;5;241m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m~/anaconda3/envs/triton/lib/python3.10/site-packages/torch/jit/_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    517\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m orig_value\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;66;03m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_concrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m cpp_module\u001b[38;5;241m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    523\u001b[0m script_module\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m scripted\n",
      "    \u001b[0;31m[... skipping similar frames: RecursiveScriptModule._construct at line 614 (1 times), create_script_module_impl at line 542 (1 times), create_script_module_impl.<locals>.init_fn at line 520 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/triton/lib/python3.10/site-packages/torch/jit/_recursive.py:542\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    539\u001b[0m     script_module\u001b[38;5;241m.\u001b[39m_concrete_type \u001b[38;5;241m=\u001b[39m concrete_type\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m script_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecursiveScriptModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpp_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m~/anaconda3/envs/triton/lib/python3.10/site-packages/torch/jit/_script.py:614\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;124;03mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m script_module \u001b[38;5;241m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> 614\u001b[0m \u001b[43minit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    618\u001b[0m RecursiveScriptModule\u001b[38;5;241m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m~/anaconda3/envs/triton/lib/python3.10/site-packages/torch/jit/_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    517\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m orig_value\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;66;03m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_concrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m cpp_module\u001b[38;5;241m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    523\u001b[0m script_module\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m scripted\n",
      "File \u001b[0;32m~/anaconda3/envs/triton/lib/python3.10/site-packages/torch/jit/_recursive.py:546\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n\u001b[0;32m--> 546\u001b[0m     \u001b[43mcreate_methods_and_properties_from_stubs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_stubs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_stubs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;66;03m# Create hooks after methods to ensure no name collisions between hooks and methods.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;66;03m# If done before, hooks can overshadow methods that aren't exported.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs)\n",
      "File \u001b[0;32m~/anaconda3/envs/triton/lib/python3.10/site-packages/torch/jit/_recursive.py:397\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    394\u001b[0m property_defs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mdef_ \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[1;32m    395\u001b[0m property_rcbs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mresolution_callback \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[0;32m--> 397\u001b[0m \u001b[43mconcrete_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_methods_and_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproperty_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defaults\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \nModule 'BertIntermediate' has no attribute 'intermediate_act_fn' (This attribute exists on the Python module, but we failed to convert Python type: 'functools.partial' to a TorchScript type. Only tensors and (possibly nested) tuples of tensors, lists, or dictsare supported as inputs or outputs of traced functions, but instead got value of type partial.. Its type was inferred; try adding a type annotation for the attribute.):\n  File \"/root/ufront/torch_bert.py\", line 343\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n                        ~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n        return hidden_states\n"
     ]
    }
   ],
   "source": [
    "import iree.compiler as ireec\n",
    "from iree import runtime\n",
    "from torch_bert import BertModel, BertConfig\n",
    "import torch\n",
    "import time\n",
    "modelname = \"Bert\"\n",
    "input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "config = BertConfig(vocab_size_or_config_json_file=16000, hidden_size=768,\n",
    "    num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "model = BertModel(config=config)\n",
    "model.eval()\n",
    "\n",
    "ts_graph = torch.jit.script(model)\n",
    "module_ir = torch_mlir.compile(ts_graph, input_ids,\n",
    "                                    output_type=torch_mlir.OutputType.LINALG_ON_TENSORS)\n",
    "\n",
    "binary = compile_to_vmfb(module_ir, target_backend=\"llvm-cpu\")\n",
    "compiled_model = runtime.load_vm_flatbuffer(binary,backend=\"llvm-cpu\")\n",
    "        \n",
    "print(\"Performing benchmark...\")\n",
    "\n",
    "t1_stop = time.perf_counter()\n",
    "print(\"**** Model {} - Total Time: {:.3f}s\".format(modelname, t1_stop - t1_start)) # print performance indicator\n",
    "\n",
    "%timeit -n 100 compiled_model.forward(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe06a06",
   "metadata": {},
   "source": [
    "#### RNN/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "179f4463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arg annotations should have one entry per function parameter (including self).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch_def import *\n",
    "modelname = \"LSTM\"\n",
    "batch_size = 8\n",
    "hidden_size = 128\n",
    "seq_size = 32\n",
    "input_size = 256\n",
    "try: \n",
    "    t1_start = time.perf_counter()\n",
    "\n",
    "    input = np.random.randn(batch_size, seq_size,hidden_size).astype(np.float32)\n",
    "    h0 = np.zeros((batch_size, hidden_size), dtype=np.float32)\n",
    "    c0 = np.zeros((batch_size, hidden_size), dtype=np.float32)\n",
    "\n",
    "    input, h0, c0 = torch.Tensor(input), torch.Tensor(h0), torch.Tensor(c0)\n",
    "    model = SimpleLSTM(input_size = 10, hidden_size = hidden_size, seq_size=seq_size)\n",
    "\n",
    "    model.train(mode=False)\n",
    "    ts_graph = torch.jit.script(model)\n",
    "    module_ir = torch_mlir.compile(ts_graph, input,\n",
    "                                        output_type=torch_mlir.OutputType.LINALG_ON_TENSORS)\n",
    "\n",
    "    binary = compile_to_vmfb(module_ir, target_backend=\"llvm-cpu\")\n",
    "    compiled_model = runtime.load_vm_flatbuffer(binary,backend=\"llvm-cpu\")\n",
    "    print(\"Performing benchmark...\")\n",
    "\n",
    "    t1_stop = time.perf_counter()\n",
    "    print(\"**** Model {} - Total Time: {:.3f}s\".format(modelname, t1_stop - t1_start)) # print performance indicator\n",
    "\n",
    "    %timeit -n 100 compiled_model.forward(input)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e0346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89bd1686",
   "metadata": {},
   "source": [
    "# UFront Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e877c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping ufront as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mProcessing ./ufront-0.1.1-cp310-cp310-manylinux_2_28_x86_64.whl\n",
      "Requirement already satisfied: onnx in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from ufront==0.1.1) (1.16.0)\n",
      "Requirement already satisfied: tf2onnx in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from ufront==0.1.1) (1.16.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from onnx->ufront==0.1.1) (1.26.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from onnx->ufront==0.1.1) (3.20.3)\n",
      "Requirement already satisfied: requests in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from tf2onnx->ufront==0.1.1) (2.31.0)\n",
      "Requirement already satisfied: six in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from tf2onnx->ufront==0.1.1) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from tf2onnx->ufront==0.1.1) (24.3.25)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from requests->tf2onnx->ufront==0.1.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from requests->tf2onnx->ufront==0.1.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from requests->tf2onnx->ufront==0.1.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from requests->tf2onnx->ufront==0.1.1) (2024.2.2)\n",
      "Installing collected packages: ufront\n",
      "Successfully installed ufront-0.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall ufront -y\n",
    "!pip install ./ufront-0.1.1-cp310-cp310-manylinux_2_28_x86_64.whl\n",
    "# !pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f876afe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/triton/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import time\n",
    "import torch\n",
    "from torchvision.models import resnet18, resnet50, squeezenet1_1, regnet_x_32gf, maxvit_t, shufflenet_v2_x1_5, inception_v3, mobilenet_v3_small, efficientnet_v2_s, densenet121, convnext_small\n",
    "import torchvision.models as models\n",
    "from ufront.pytorch.model import UFrontTorch\n",
    "import argparse\n",
    "import ctypes\n",
    "from iree.compiler import tools\n",
    "from iree import runtime\n",
    "import iree.runtime as ireert\n",
    "import iree.compiler as ireec\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import iree.runtime as ireert\n",
    "import iree.compiler as ireec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c90483",
   "metadata": {},
   "source": [
    "#### Vision models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ddbd21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3****Ufront->TOSA Time: 0.240s, TOSA->Binary Time: 2.635s, Total Time: 2.875s\n",
      "MobileNetV3 - 1.350 ± 0.000 ms\n",
      "ShuffleNetV2****Ufront->TOSA Time: 0.345s, TOSA->Binary Time: 1.923s, Total Time: 2.268s\n",
      "ShuffleNetV2 - 2.420 ± 0.000 ms\n",
      "ResNet18****Ufront->TOSA Time: 0.362s, TOSA->Binary Time: 1.696s, Total Time: 2.059s\n",
      "ResNet18 - 2.891 ± 0.008 ms\n",
      "ResNet50****Ufront->TOSA Time: 0.777s, TOSA->Binary Time: 3.141s, Total Time: 3.918s\n",
      "ResNet50 - 6.039 ± 0.028 ms\n",
      "SqueezeNet****Ufront->TOSA Time: 0.064s, TOSA->Binary Time: 1.300s, Total Time: 1.364s\n",
      "SqueezeNet - 1.108 ± 0.004 ms\n",
      "DenseNet121****Ufront->TOSA Time: 1.203s, TOSA->Binary Time: 5.280s, Total Time: 6.483s\n",
      "DenseNet121 - 7.640 ± 0.004 ms\n",
      "InceptionV3****Ufront->TOSA Time: 0.957s, TOSA->Binary Time: 4.099s, Total Time: 5.056s\n",
      "InceptionV3 - 12.080 ± 0.040 ms\n",
      "ViT_B16****Ufront->TOSA Time: 2.399s, TOSA->Binary Time: 4.911s, Total Time: 7.311s\n",
      "ViT_B16 - 29.400 ± 0.089 ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from benchmark import benchmark_module\n",
    "batch_size = 1\n",
    "input_sample = np.random.uniform(low=0.0, high=1.0, size=(batch_size, 3, 224, 224)).astype(np.float32)\n",
    "input = torch.Tensor(input_sample)\n",
    "\n",
    "model_list = {\"MobileNetV3\":mobilenet_v3_small(pretrained=False), \"ShuffleNetV2\":shufflenet_v2_x1_5(pretrained=False),\n",
    "            \"ResNet18\":resnet18(pretrained=False), \"ResNet50\":resnet50(pretrained=False), \"SqueezeNet\":squeezenet1_1(pretrained=False),\n",
    "            \"DenseNet121\":densenet121(pretrained=False), \"InceptionV3\":inception_v3(pretrained=False), \"ViT_B16\":models.vision_transformer.vit_b_16(weights=False, dropout=0.1)}\n",
    "\n",
    "for modelname, net in model_list.items():\n",
    "    net.train(False) \n",
    "\n",
    "    t1_start = time.perf_counter()\n",
    "    model = UFrontTorch(net, batch_size=batch_size, pass_weights=True) # convert torch model to ufront model\n",
    "    #This will trigger Rust frontend for actual model conversion and graph building\n",
    "    #operators can also be managed by python side (each operator here corresponding to an operator in the Rust computation graph)\n",
    "    output_tensors = model(inputs = [input])\n",
    "\n",
    "    #This will trigger model compilation, i.e., convert Rust computation graph to a unified high-level IR and lower it to TOSA IR\n",
    "    model.compile(optimizer={\"type\":\"sgd\", \"lr\":\"0.01\", \"momentum\":\"0\", \"nesterov\":\"False\", \"weight_decay\":\"0\"},\n",
    "                        loss='sparse_categorical_crossentropy', metrics=['accuracy', 'sparse_categorical_crossentropy'])\n",
    "    tosa_ir= model.dump_tosa_ir()\n",
    "\n",
    "    t1_stop = time.perf_counter()\n",
    "\n",
    "    binary = ireec.compile_str(tosa_ir,\n",
    "                    target_backends=[\"cuda\"], \n",
    "                    input_type=ireec.InputType.TOSA)\n",
    "    t2_stop = time.perf_counter()\n",
    "\n",
    "    print(modelname + \"****Ufront->TOSA Time: {:.3f}s, TOSA->Binary Time: {:.3f}s, Total Time: {:.3f}s\".format(t1_stop - t1_start, t2_stop - t1_stop, t2_stop - t1_start)) # print performance indicator\n",
    "    module = runtime.load_vm_flatbuffer(binary, driver=\"cuda\")\n",
    "    tms = []\n",
    "    for i in range(10):\n",
    "        ret = benchmark_module(module.vm_module, entry_function=\"forward\", inputs=[\"1x3x224x224xf32=1\"], device=\"cuda\")\n",
    "        tm = ret[0].time\n",
    "        tms.append(float(tm[0:-3]))\n",
    "    print(\"{} - {:.3f} ± {:.3f} ms\".format(modelname, np.mean(tms), np.std(tms)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cadb85c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "684ffd50",
   "metadata": {},
   "source": [
    "#### Language Model (Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "327f24a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling TOSA model...\n",
      "Compiling Binary...\n",
      "Bert****Ufront->TOSA Time: 3.406s, TOSA->Binary Time: 4.651s, Total Time: 8.057s\n",
      "3.41 ms ± 30.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "from ufront.pytorch.model import UFrontTorch \n",
    "import iree.compiler as ireec\n",
    "from iree import runtime\n",
    "from torch_bert import BertModel, BertConfig\n",
    "import torch\n",
    "import time\n",
    "input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "config = BertConfig(vocab_size_or_config_json_file=16000, hidden_size=768,\n",
    "    num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "net = BertModel(config=config)\n",
    "net.eval()\n",
    "\n",
    "t1_start = time.perf_counter()\n",
    "model = UFrontTorch(net, batch_size=1, pass_weights=True) # convert torch model to ufront model\n",
    "output_tensors = model(inputs = [input_ids, token_type_ids, input_mask])\n",
    "\n",
    "model.compile(optimizer={\"type\":\"sgd\", \"lr\":\"0.01\", \"momentum\":\"0\", \"nesterov\":\"False\", \"weight_decay\":\"0\"},\n",
    "                    loss='sparse_categorical_crossentropy', metrics=['accuracy', 'sparse_categorical_crossentropy'])\n",
    "\n",
    "print(\"Compiling TOSA model...\")\n",
    "tosa_ir= model.dump_tosa_ir()\n",
    "t1_stop = time.perf_counter()\n",
    "print(\"Compiling Binary...\")\n",
    "binary = ireec.compile_str(tosa_ir,\n",
    "                target_backends=[\"cuda\"], \n",
    "                input_type=ireec.InputType.TOSA)\n",
    "t2_stop = time.perf_counter()\n",
    "print(\"Bert****Ufront->TOSA Time: {:.3f}s, TOSA->Binary Time: {:.3f}s, Total Time: {:.3f}s\".format(t1_stop - t1_start, t2_stop - t1_stop, t2_stop - t1_start)) # print performance indicator\n",
    "module = runtime.load_vm_flatbuffer(binary, driver=\"cuda\")\n",
    "\n",
    "%timeit -n 100 module.forward(input_ids, token_type_ids, input_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aff4f3",
   "metadata": {},
   "source": [
    "#### RNN/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59bf17f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM****Ufront->TOSA Time: 0.066s, TOSA->Binary Time: 1.468s, Total Time: 1.534s\n",
      "1.17 ms ± 62.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ufront.pytorch.model import UFrontTorch\n",
    "import iree.compiler as ireec\n",
    "from iree import runtime\n",
    "import time\n",
    "import torch\n",
    "from torch_def import *\n",
    "import numpy as np\n",
    "batch_size = 8\n",
    "hidden_size = 128\n",
    "seq_size = 32\n",
    "input_size = 256\n",
    "input = np.random.randn(batch_size, seq_size,hidden_size).astype(np.float32)\n",
    "h0 = np.zeros((batch_size, hidden_size), dtype=np.float32)\n",
    "c0 = np.zeros((batch_size, hidden_size), dtype=np.float32)\n",
    "t1_start = time.perf_counter()\n",
    "input, h0, c0 = torch.Tensor(input), torch.Tensor(h0), torch.Tensor(c0)\n",
    "lstm = SimpleLSTM(input_size = 10, hidden_size = hidden_size, seq_size=seq_size)\n",
    "model = UFrontTorch(lstm, batch_size=batch_size, pass_weights=True)\n",
    "output_tensors = model(inputs = [input, h0, c0])\n",
    "model.compile(optimizer={\"type\":\"sgd\", \"lr\":\"0.01\", \"momentum\":\"0\", \"nesterov\":\"False\", \"weight_decay\":\"0\"},\n",
    "                      loss='sparse_categorical_crossentropy', metrics=['accuracy', 'sparse_categorical_crossentropy'])\n",
    "\n",
    "tosa_ir = model.dump_tosa_ir()\n",
    "t1_stop = time.perf_counter()\n",
    "\n",
    "binary = ireec.compile_str(tosa_ir,\n",
    "                target_backends=[\"cuda\"], \n",
    "                input_type=ireec.InputType.TOSA)\n",
    "t2_stop = time.perf_counter()\n",
    "\n",
    "print(\"LSTM****Ufront->TOSA Time: {:.3f}s, TOSA->Binary Time: {:.3f}s, Total Time: {:.3f}s\".format(t1_stop - t1_start, t2_stop - t1_stop, t2_stop - t1_start)) # print performance indicator\n",
    "\n",
    "module = runtime.load_vm_flatbuffer(binary, driver=\"cuda\")\n",
    "%timeit -n 100 module.forward(input, h0, c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a7116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
