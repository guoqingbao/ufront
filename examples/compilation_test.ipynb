{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db63d783-79ed-4f61-b064-eabc0c958fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import time\n",
    "import torch\n",
    "from torchvision.models import resnet18, resnet50, squeezenet1_1, regnet_x_32gf, maxvit_t, shufflenet_v2_x1_5, inception_v3, mobilenet_v3_small, efficientnet_v2_s, densenet121, convnext_small\n",
    "import torchvision.models as models\n",
    "from ufront.pytorch.model import UFrontTorch\n",
    "import argparse\n",
    "import ctypes\n",
    "# pip install iree-compiler iree-runtime iree-tools-tf -f https://openxla.github.io/iree/pip-release-links.html\n",
    "from iree.compiler import tools\n",
    "from iree import runtime\n",
    "import iree.runtime as ireert\n",
    "import iree.compiler as ireec\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import iree.runtime as ireert\n",
    "import iree.compiler as ireec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a1e75-1c65-43bb-8c0c-4717c7c4004f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80bf7638-4aff-4f50-ab1c-abfdddd00977",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3****Ufront->TOSA Time: 0.274s, TOSA->Binary Time: 8.913s, Total Time: 9.187s\n",
      "ShuffleNetV2****Ufront->TOSA Time: 0.413s, TOSA->Binary Time: 5.769s, Total Time: 6.182s\n",
      "ResNet18****Ufront->TOSA Time: 1.481s, TOSA->Binary Time: 3.935s, Total Time: 5.416s\n",
      "ResNet50****Ufront->TOSA Time: 2.993s, TOSA->Binary Time: 7.321s, Total Time: 10.314s\n",
      "SqueezeNet****Ufront->TOSA Time: 0.172s, TOSA->Binary Time: 3.415s, Total Time: 3.587s\n",
      "DenseNet121****Ufront->TOSA Time: 1.051s, TOSA->Binary Time: 23.630s, Total Time: 24.681s\n",
      "InceptionV3****Ufront->TOSA Time: 2.862s, TOSA->Binary Time: 10.515s, Total Time: 13.377s\n",
      "ViT_B16****Ufront->TOSA Time: 6.293s, TOSA->Binary Time: 5.702s, Total Time: 11.996s\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "input = torch.ones((batch_size, 3, 224, 224), dtype=torch.float32)\n",
    "\n",
    "model_list = {\"MobileNetV3\":mobilenet_v3_small(pretrained=False), \"ShuffleNetV2\":shufflenet_v2_x1_5(pretrained=False),\n",
    "            \"ResNet18\":resnet18(pretrained=False), \"ResNet50\":resnet50(pretrained=False), \"SqueezeNet\":squeezenet1_1(pretrained=False),\n",
    "            \"DenseNet121\":densenet121(pretrained=False), \"InceptionV3\":inception_v3(pretrained=False), \"ViT_B16\":models.vision_transformer.vit_b_16(weights=False, dropout=0.1)}\n",
    "\n",
    "for modelname, net in model_list.items():\n",
    "    # blockPrint()\n",
    "    net.train(False) \n",
    "\n",
    "    t1_start = time.perf_counter()\n",
    "    model = UFrontTorch(net, batch_size=batch_size) # convert torch model to ufront model\n",
    "    #This will trigger Rust frontend for actual model conversion and graph building\n",
    "    #operators can also be managed by python side (each operator here corresponding to an operator in the Rust computation graph)\n",
    "    output_tensors = model(inputs = [input])\n",
    "\n",
    "    #The output of the model (forward pass have not been triggered at the moment!)\n",
    "    if model.model.__class__.__name__ not in [\"MaxVit\", \"SwinTransformer\", \"VisionTransformer\", \"MultiHeadAttention\"]:\n",
    "        output = model.softmax(input=output_tensors[0], name=\"softmax_out\")\n",
    "\n",
    "    #This will trigger model compilation, i.e., convert Rust computation graph to a unified high-level IR and lower it to TOSA IR\n",
    "    model.compile(optimizer={\"type\":\"sgd\", \"lr\":\"0.01\", \"momentum\":\"0\", \"nesterov\":\"False\", \"weight_decay\":\"0\"},\n",
    "                        loss='sparse_categorical_crossentropy', metrics=['accuracy', 'sparse_categorical_crossentropy'])\n",
    "    tosa_ir= model.dump_tosa_ir()\n",
    "\n",
    "    t1_stop = time.perf_counter()\n",
    "\n",
    "    # print(\"Compiling TOSA model...\")\n",
    "    compiled_module = ireec.compile_str(tosa_ir,\n",
    "                        target_backends=[\"llvm-cpu\"],\n",
    "                        input_type=ireec.InputType.TOSA)\n",
    "\n",
    "    t2_stop = time.perf_counter()\n",
    "\n",
    "    print(modelname + \"****Ufront->TOSA Time: {:.3f}s, TOSA->Binary Time: {:.3f}s, Total Time: {:.3f}s\".format(t1_stop - t1_start, t2_stop - t1_stop, t2_stop - t1_start)) # print performance indicator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5c14b5d-ae7e-4064-9425-eafaff1b4c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling TOSA model...\n",
      "884101340\n",
      "Compiling Binary...\n",
      "Bert****Ufront->TOSA Time: 3.426s, TOSA->Binary Time: 95.340s, Total Time: 98.765s\n"
     ]
    }
   ],
   "source": [
    "# import torch, torchtext\n",
    "from ufront.pytorch.model import UFrontTorch \n",
    "import iree.compiler as ireec\n",
    "from iree import runtime\n",
    "from torch_bert import BertModel, BertConfig\n",
    "import torch\n",
    "import time\n",
    "GPU = True\n",
    "input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "    num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "net = BertModel(config=config)\n",
    "net.eval()\n",
    "\n",
    "t1_start = time.perf_counter()\n",
    "model = UFrontTorch(net, batch_size=1, pass_weights=True) # convert torch model to ufront model\n",
    "#This will trigger Rust frontend for actual model conversion and graph building\n",
    "#operators can also be managed by python side (each operator here corresponding to an operator in the Rust computation graph)\n",
    "output_tensors = model(inputs = [input_ids, token_type_ids, input_mask])\n",
    "\n",
    "#This will trigger model compilation, i.e., convert Rust computation graph to a unified high-level IR and lower it to TOSA IR\n",
    "model.compile(optimizer={\"type\":\"sgd\", \"lr\":\"0.01\", \"momentum\":\"0\", \"nesterov\":\"False\", \"weight_decay\":\"0\"},\n",
    "                    loss='sparse_categorical_crossentropy', metrics=['accuracy', 'sparse_categorical_crossentropy'])\n",
    "\n",
    "# modelir = model.dump_ir()\n",
    "\n",
    "# print(modelir)\n",
    "\n",
    "# import pathlib\n",
    "# path = str(pathlib.Path(__file__).parent.resolve()) + \"/output_ir/torch_\" + model.model.__class__.__name__ + \".ir\"\n",
    "# path = str(pathlib.Path(__file__).parent.resolve()) + \"/output_ir/torch_Resnet18.ir\"\n",
    "\n",
    "# f = open(path, \"w\")\n",
    "# f.write(modelir)\n",
    "# f.close()\n",
    "\n",
    "# print(\"\\r\\n\\r\\nIR for \", model.model.__class__.__name__, \" generated: \", path)\n",
    "\n",
    "print(\"Compiling TOSA model...\")\n",
    "tosa_ir= model.dump_tosa_ir()\n",
    "t1_stop = time.perf_counter()\n",
    "\n",
    "print(len(tosa_ir))\n",
    "\n",
    "print(\"Compiling Binary...\")\n",
    "\n",
    "if GPU:\n",
    "    binary = ireec.compile_str(tosa_ir,\n",
    "                    target_backends=[\"cuda\"], \n",
    "                    input_type=ireec.InputType.TOSA)\n",
    "    module = runtime.load_vm_flatbuffer(binary, driver=\"cuda\")\n",
    "else:\n",
    "    binary = ireec.compile_str(tosa_ir,\n",
    "                    target_backends=[\"llvm-cpu\"], \n",
    "                    input_type=ireec.InputType.TOSA)\n",
    "    module = runtime.load_vm_flatbuffer(binary,backend=\"llvm-cpu\") \n",
    "\n",
    "t2_stop = time.perf_counter()\n",
    "\n",
    "print(\"Bert****Ufront->TOSA Time: {:.3f}s, TOSA->Binary Time: {:.3f}s, Total Time: {:.3f}s\".format(t1_stop - t1_start, t2_stop - t1_stop, t2_stop - t1_start)) # print performance indicator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "316caf0d-dddc-4940-bb0a-1794d96699c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.22 ms ± 18.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%timeit -n 100 module.forward(input_ids, token_type_ids, input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f9d07a-5c3b-4c41-b393-86ed187a9e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c456455d-d8d3-4c4d-be67-508c05b6403d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "749af7b4-86d4-4c88-99ee-fd3503c6a44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling TOSA model...\n",
      "880918944\n",
      "Compiling Binary...\n",
      "Bert****Ufront->TOSA Time: 21.993s, TOSA->Binary Time: 5.739s, Total Time: 27.732s\n"
     ]
    }
   ],
   "source": [
    "# import torch, torchtext\n",
    "from ufront.pytorch.model import UFrontTorch \n",
    "import iree.compiler as ireec\n",
    "from iree import runtime\n",
    "from bert import BertModel, BertConfig\n",
    "import torch\n",
    "import time\n",
    "from ufront.onnx.model import ONNXModel, ONNXModelKeras, UFrontONNX\n",
    "from torch.onnx import TrainingMode\n",
    "import onnx\n",
    "import io\n",
    "GPU = True\n",
    "input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "    num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "net = BertModel(config=config)\n",
    "net.eval()\n",
    "t1_start = time.perf_counter()\n",
    "\n",
    "f = io.BytesIO()\n",
    "model_name = net.__class__.__name__ \n",
    "torch.onnx.export(model=net, args=(input_ids, input_mask, token_type_ids), f=f, export_params=True, #do_constant_folding=True,\n",
    "                    training=TrainingMode.EVAL if model_name==\"Inception3\" else TrainingMode.TRAINING, opset_version=17)\n",
    "onnx_model = onnx.load_model_from_string(f.getvalue())\n",
    "\n",
    "# transformer = True if model_name in [\"MaxVit\", \"SwinTransformer\", \"VisionTransformer\", \"MultiHeadAttention\"] else False\n",
    "model = UFrontONNX(onnx_model=onnx_model, batch_size=1, simplify=True, pass_weights=True, transformer=True)\n",
    "\n",
    "\n",
    "#operators can also be managed by python side (each operator here corresponding to an operator in the Rust computation graph)\n",
    "output_tensors = model(inputs = [input_ids, token_type_ids, input_mask])\n",
    "\n",
    "#This will trigger model compilation, i.e., convert Rust computation graph to a unified high-level IR and lower it to TOSA IR\n",
    "model.compile(optimizer={\"type\":\"sgd\", \"lr\":\"0.01\", \"momentum\":\"0\", \"nesterov\":\"False\", \"weight_decay\":\"0\"},\n",
    "                    loss='sparse_categorical_crossentropy', metrics=['accuracy', 'sparse_categorical_crossentropy'])\n",
    "\n",
    "modelir = model.dump_ir()\n",
    "\n",
    "\n",
    "print(\"Compiling TOSA model...\")\n",
    "tosa_ir= model.dump_tosa_ir()\n",
    "t1_stop = time.perf_counter()\n",
    "\n",
    "print(len(tosa_ir))\n",
    "\n",
    "\n",
    "print(\"Compiling Binary...\")\n",
    "\n",
    "if GPU:\n",
    "    binary = ireec.compile_str(tosa_ir,\n",
    "                    target_backends=[\"cuda\"], \n",
    "                    input_type=ireec.InputType.TOSA)\n",
    "    module = runtime.load_vm_flatbuffer(binary, driver=\"cuda\")\n",
    "else:\n",
    "    binary = ireec.compile_str(tosa_ir,\n",
    "                    target_backends=[\"llvm-cpu\"], \n",
    "                    input_type=ireec.InputType.TOSA)\n",
    "    module = runtime.load_vm_flatbuffer(binary,backend=\"llvm-cpu\") \n",
    "\n",
    "t2_stop = time.perf_counter()\n",
    "\n",
    "print(\"Bert****Ufront->TOSA Time: {:.3f}s, TOSA->Binary Time: {:.3f}s, Total Time: {:.3f}s\".format(t1_stop - t1_start, t2_stop - t1_stop, t2_stop - t1_start)) # print performance indicator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56d4c9e8-06da-4898-9177-95caadbe8299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.22 ms ± 50.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%timeit -n 100 module.forward(input_ids, token_type_ids, input_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150bcb90-2223-48d9-af6c-9e6555cc0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install bert-for-tf2 (need to fix some issue under TF 2.6+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ad63ba2-0492-46d4-881d-d3d305540268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert import BertModelLayer\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from ufront.keras.model import UFrontKeras\n",
    "\n",
    "l_bert = BertModelLayer(**BertModelLayer.Params(\n",
    "  vocab_size               = 16000,        # embedding params\n",
    "  use_token_type           = True,\n",
    "  use_position_embeddings  = True,\n",
    "  token_type_vocab_size    = 2,\n",
    "\n",
    "  num_layers               = 12,           # transformer encoder params\n",
    "  hidden_size              = 768,\n",
    "  hidden_dropout           = 0.1,\n",
    "  intermediate_size        = 4*768,\n",
    "  intermediate_activation  = \"gelu\",\n",
    "\n",
    "  adapter_size             = None,         # see arXiv:1902.00751 (adapter-BERT)\n",
    "\n",
    "  shared_layer             = False,        # True for ALBERT (arXiv:1909.11942)\n",
    "  embedding_size           = None,         # None for BERT, wordpiece embedding size for ALBERT\n",
    "  num_heads = 12,\n",
    "  # name                     = \"bert\"        # any other Keras layer params\n",
    "))\n",
    "\n",
    "input_ids = np.array([[31, 51, 99], [15, 5, 0]], dtype='int64')\n",
    "input_mask = np.array([[1, 1, 1], [1, 1, 0]], dtype='int64')\n",
    "token_type_ids = np.array([[0, 0, 1], [0, 1, 0]], dtype='int64')\n",
    "\n",
    "max_seq_len = 128\n",
    "l_input_ids      = keras.layers.Input(shape=(max_seq_len,), dtype='int64')\n",
    "l_token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int64')\n",
    "\n",
    "output = l_bert([l_input_ids, l_token_type_ids])          # [batch_size, max_seq_len, hidden_size]\n",
    "net = keras.Model(inputs=[l_input_ids, l_token_type_ids], outputs=output)\n",
    "\n",
    "#build UFront model\n",
    "ufront_model = UFrontKeras(net, inputs = [input_ids, token_type_ids], batch_size = 1, transformer=True, pass_weights=True)\n",
    "\n",
    "\n",
    "ufront_model.compile(optimizer={\"type\":\"sgd\", \"lr\":\"0.01\", \"momentum\":\"0\", \"nesterov\":\"False\", \"weight_decay\":\"0\"},\n",
    "                        loss='sparse_categorical_crossentropy', metrics=['accuracy', 'sparse_categorical_crossentropy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
