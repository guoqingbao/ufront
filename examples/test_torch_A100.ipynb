{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ccb3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 22 14:49:51 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-PCIE-40GB          Off | 00000000:3D:00.0 Off |                    0 |\n",
      "| N/A   30C    P0              35W / 250W |   1185MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-PCIE-40GB          Off | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   30C    P0              33W / 250W |      7MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-PCIE-40GB          Off | 00000000:40:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              33W / 250W |      7MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-PCIE-40GB          Off | 00000000:41:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              33W / 250W |      7MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-PCIE-40GB          Off | 00000000:DA:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              34W / 250W |  37085MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-PCIE-40GB          Off | 00000000:DB:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              38W / 250W |  37085MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-PCIE-40GB          Off | 00000000:DD:00.0 Off |                    0 |\n",
      "| N/A   31C    P0              33W / 250W |    103MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-PCIE-40GB          Off | 00000000:DE:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              34W / 250W |      7MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2ac8c4",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b816d1",
   "metadata": {},
   "source": [
    "#### Install IREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbeb32e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: iree-compiler==20230815.614 in /opt/conda/lib/python3.10/site-packages (20230815.614)\n",
      "Requirement already satisfied: iree-runtime==20230815.614 in /opt/conda/lib/python3.10/site-packages (20230815.614)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from iree-compiler==20230815.614) (1.24.3)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from iree-compiler==20230815.614) (6.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: iree-tools-tf==20230815.614 in /opt/conda/lib/python3.10/site-packages (20230815.614)\n",
      "Requirement already satisfied: iree-tools-tflite==20230815.614 in /opt/conda/lib/python3.10/site-packages (20230815.614)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# For CUDA 11\n",
    "# !pip install iree-compiler==20230524.529 iree-runtime==20230524.529 \n",
    "# !pip install iree-tools-tf==20230524.529  iree-tools-tflite==20230524.529\n",
    "\n",
    "# For CUDA 12\n",
    "\n",
    "!pip install iree-compiler==20230815.614 iree-runtime==20230815.614\n",
    "!pip install iree-tools-tf==20230815.614  iree-tools-tflite==20230815.614\n",
    "# fix issue of iree-benchmark-module for iree-compiler (v20230815.614), depend on the installation of IREE package\n",
    "# ls /opt/conda/lib/python3.10/site-packages/iree/_runtime_libs/\n",
    "# cp /opt/conda/lib/python3.10/site-packages/iree/_runtime_libs/iree-benchmark-module /opt/conda/lib/python3.10/site-packages/iree/runtime/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac8f274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iree-compiler                20230815.614\n",
      "iree-runtime                 20230815.614\n",
      "iree-tools-tf                20230815.614\n",
      "iree-tools-tflite            20230815.614\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep iree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a10c935",
   "metadata": {},
   "source": [
    "#### Install Torch-MLIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b94a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/llvm/torch-mlir/releases/tag/snapshot-20230525.849 download and install torch-mlir and corresponding torch-cpu\n",
    "# !pip install torch-mlir==20230525.849 -f https://github.com/llvm/torch-mlir/releases/download/snapshot-20230525.849/torch_mlir-20230525.849-cp310-cp310-linux_x86_64.whl --no-dependencies\n",
    "# !pip install torch==2.1.0.dev20230523 -f https://github.com/llvm/torch-mlir/releases/download/snapshot-20230525.849/torch-2.1.0.dev20230523+cpu-cp310-cp310-linux_x86_64.whl --no-dependencies\n",
    "# !pip install torchvision==0.16.0 --no-dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bdd99ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                        2.1.0.dev20230523+cpu\n",
      "torch-mlir                   20230525.849\n",
      "torchaudio                   2.1.0\n",
      "torchelastic                 0.2.2\n",
      "torchvision                  0.16.0\n"
     ]
    }
   ],
   "source": [
    "# torch-mlir requires matched torch (dev)\n",
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d644b6a",
   "metadata": {},
   "source": [
    "# Torch-MLIR Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42fb8162-39d8-4167-9ffd-11b25bdcc161",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import io\n",
    "import numpy as np\n",
    "import time\n",
    "import torch_mlir\n",
    "from torchvision.models import resnet18, resnet50, squeezenet1_1, regnet_x_32gf, maxvit_t, shufflenet_v2_x1_5, inception_v3, mobilenet_v3_small, efficientnet_v2_s, densenet121, convnext_small\n",
    "import torchvision.models as models\n",
    "from iree import runtime\n",
    "from typing import Optional\n",
    "from torch.utils._pytree import tree_map\n",
    "import iree.runtime as ireert\n",
    "import iree.compiler as ireec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc40ecf0-0f53-465b-9911-5c54c15b9b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IREEInvoker:\n",
    "    \"\"\"A wrapper around an IREE module that provides a Pythonic interface.\n",
    "    \n",
    "    Specifically, this adapts `module.forward(...)` and similar calls into\n",
    "    lower-level calls into the functions in the IREE module, and also converts\n",
    "    between the IREE and Torch types.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, iree_module):\n",
    "        self._iree_module = iree_module\n",
    "        self.device = iree_module._context.config.device\n",
    "\n",
    "    def __getattr__(self, function_name: str):\n",
    "        def invoke(*args):\n",
    "            def wrap(x):\n",
    "                if isinstance(x, torch.Tensor):\n",
    "                    return ireert.asdevicearray(self.device, x)\n",
    "                return x\n",
    "            def unwrap(x):\n",
    "                if isinstance(x, ireert.DeviceArray):\n",
    "                    return torch.from_numpy(np.asarray(x).copy())\n",
    "                return x\n",
    "            iree_args = tree_map(wrap, args)\n",
    "            result = self._iree_module[function_name](*iree_args)\n",
    "            return tree_map(unwrap, result)\n",
    "        return invoke\n",
    "    \n",
    "def _map_target_backend_to_driver(target_backend):\n",
    "    if target_backend == \"cuda\":\n",
    "        return \"cuda\"\n",
    "    if target_backend == \"vulkan\":\n",
    "        return \"vulkan\"\n",
    "    if target_backend in (\"llvm-cpu\", \"vmvx\"):\n",
    "        return \"local-sync\"\n",
    "    raise ValueError(f\"Unknown target backend: {target_backend}\")\n",
    "\n",
    "def load_vmfb(flatbuffer, backend=\"llvm-cpu\"):\n",
    "    \"\"\"Load an IREE Flatbuffer into an in-process runtime wrapper.\n",
    "    The wrapper accepts and returns `torch.Tensor` types.\n",
    "    \"\"\"\n",
    "    config = ireert.Config(driver_name=_map_target_backend_to_driver(backend))\n",
    "    ctx = ireert.SystemContext(config=config)\n",
    "    vm_module = ireert.VmModule.from_flatbuffer(ctx.instance, flatbuffer)\n",
    "    ctx.add_vm_module(vm_module)\n",
    "    return IREEInvoker(ctx.modules.module)\n",
    "\n",
    "def compile_to_vmfb(mlir_module, target_backend=\"llvm-cpu\", \n",
    "                    cuda_llvm_target_arch: Optional[str] = None):\n",
    "    \"\"\"Compile an MLIR module to an IREE Flatbuffer.\n",
    "    The module is expected to be in the format produced by `torch_mlir.compile`\n",
    "    with `OutputType.LINALG_ON_TENSORS`.\n",
    "    TODO: Expose more compiler options.\n",
    "    \"\"\"\n",
    "    extra_args = []\n",
    "    if cuda_llvm_target_arch is not None:\n",
    "        arch_flag = f\"--iree-hal-cuda-llvm-target-arch={cuda_llvm_target_arch}\"\n",
    "        extra_args.append(arch_flag)\n",
    "    bytecode_stream = io.BytesIO()\n",
    "    mlir_module.operation.write_bytecode(bytecode_stream)\n",
    "    bytecode = bytecode_stream.getvalue()\n",
    "    \n",
    "    return ireec.compile_str(bytecode,\n",
    "                             target_backends=[target_backend],\n",
    "                             input_type=ireec.InputType.TM_TENSOR,\n",
    "                             extra_args=extra_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b96c9eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6f07091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b5cd39",
   "metadata": {},
   "source": [
    "#### Vision Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b4c0c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**********Processing model MobileNetV3\n",
      "MobileNetV3****Compilation Time: 8.148s\n",
      "Calculating forward latency:\n",
      "  MobileNetV3 - 1.439 ± 0.005 ms\n",
      "\n",
      "**********Processing model ShuffleNetV2\n",
      "Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:\n",
      "\n",
      "\n",
      "python exceptio\n",
      "\n",
      "**********Processing model ResNet18\n",
      "ResNet18****Compilation Time: 3.575s\n",
      "Calculating forward latency:\n",
      "  ResNet18 - 3.691 ± 0.003 ms\n",
      "\n",
      "**********Processing model ResNet50\n",
      "ResNet50****Compilation Time: 6.779s\n",
      "Calculating forward latency:\n",
      "  ResNet50 - 11.130 ± 0.046 ms\n",
      "\n",
      "**********Processing model SqueezeNet\n",
      "SqueezeNet****Compilation Time: 3.450s\n",
      "Calculating forward latency:\n",
      "  SqueezeNet - 1.228 ± 0.004 ms\n",
      "\n",
      "**********Processing model DenseNet121\n",
      "Lowering Torch Backend IR -> Linalg-on-Tensors Backend IR failed with the following diagnostics:\n",
      "\n",
      "\n",
      "p\n",
      "\n",
      "**********Processing model InceptionV3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/jit/_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:\n",
      "\n",
      "\n",
      "python exceptio\n",
      "\n",
      "**********Processing model ViT_B16\n",
      "Lowering TorchScript IR -> Torch Backend IR failed with the following diagnostics:\n",
      "\n",
      "\n",
      "python exceptio\n"
     ]
    }
   ],
   "source": [
    "from benchmark import benchmark_module\n",
    "batch_size = 1\n",
    "input_sample = np.random.uniform(low=0.0, high=1.0, size=(batch_size, 3, 224, 224)).astype(np.float32)\n",
    "input = torch.Tensor(input_sample)\n",
    "model_list = {\"MobileNetV3\":mobilenet_v3_small(pretrained=False), \"ShuffleNetV2\":shufflenet_v2_x1_5(pretrained=False),\n",
    "            \"ResNet18\":resnet18(pretrained=False), \"ResNet50\":resnet50(pretrained=False), \"SqueezeNet\":squeezenet1_1(pretrained=False),\n",
    "            \"DenseNet121\":densenet121(pretrained=False), \"InceptionV3\":inception_v3(pretrained=False), \"ViT_B16\":models.vision_transformer.vit_b_16(weights=False)}\n",
    "\n",
    "for modelname, model in model_list.items():\n",
    "    print(\"\\r\\n**********Processing model \" + modelname)\n",
    "    try: \n",
    "        model.train(mode=False)\n",
    "        t1_start = time.perf_counter()\n",
    "        \n",
    "        ts_graph = torch.jit.script(model)\n",
    "        module_ir = torch_mlir.compile(ts_graph, input,\n",
    "                                            output_type=torch_mlir.OutputType.LINALG_ON_TENSORS)\n",
    "\n",
    "        binary = compile_to_vmfb(module_ir, target_backend=\"cuda\")\n",
    "\n",
    "        t2_stop = time.perf_counter()\n",
    "        module = runtime.load_vm_flatbuffer(binary, driver=\"cuda\")\n",
    "\n",
    "        print(modelname + \"****Compilation Time: {:.3f}s\".format(t2_stop - t1_start)) # print performance indicator\n",
    "\n",
    "        print(\"Calculating forward latency:\\n  \", end=\"\")\n",
    "        tms = []\n",
    "        for i in range(10):\n",
    "            ret = benchmark_module(module.vm_module, entry_function=\"forward\", inputs=[\"1x3x224x224xf32=1\"], device=\"cuda\")\n",
    "            tm = ret[0].time\n",
    "            tms.append(float(tm[0:-3]))\n",
    "        print(\"{} - {:.3f} ± {:.3f} ms\".format(modelname, np.mean(tms), np.std(tms)))\n",
    "    except Exception as e:\n",
    "        print(str(e)[:100]) #print error head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa825e8",
   "metadata": {},
   "source": [
    "#### Language Model (Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eda0266",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\nArguments for call are not valid.\nThe following variants are available:\n  \n  aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor:\n  Expected a value of type 'Optional[Device]' for argument 'device' but instead found type 'Tensor (inferred)'.\n  Inferred the value for argument 'device' to be of type 'Tensor' because it was not annotated with an explicit type.\n  \n  aten::arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor:\n  Argument end not provided.\n  \n  aten::arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor:\n  Argument end not provided.\n  \n  aten::arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!):\n  Argument end not provided.\n  \n  aten::arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!):\n  Argument out not provided.\n\nThe original call is:\n  File \"/root/ufront-test/torch_bert.py\", line 221\n    def forward(self, x, dtype, device):\n        return torch.arange(x, dtype=dtype, device=device)\n               ~~~~~~~~~~~~ <--- HERE\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m BertModel(config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 17\u001b[0m ts_graph \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m module_ir \u001b[38;5;241m=\u001b[39m torch_mlir\u001b[38;5;241m.\u001b[39mcompile(ts_graph, input_ids,\n\u001b[1;32m     19\u001b[0m                                     output_type\u001b[38;5;241m=\u001b[39mtorch_mlir\u001b[38;5;241m.\u001b[39mOutputType\u001b[38;5;241m.\u001b[39mLINALG_ON_TENSORS)\n\u001b[1;32m     21\u001b[0m binary \u001b[38;5;241m=\u001b[39m compile_to_vmfb(module_ir, target_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllvm-cpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_script.py:1284\u001b[0m, in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m   1283\u001b[0m     obj \u001b[38;5;241m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[0;32m-> 1284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_script_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_methods_to_compile\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m create_script_dict(obj)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_recursive.py:480\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing:\n\u001b[1;32m    479\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[38;5;241m.\u001b[39mcheck(nn_module)\n\u001b[0;32m--> 480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_recursive.py:542\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    539\u001b[0m     script_module\u001b[38;5;241m.\u001b[39m_concrete_type \u001b[38;5;241m=\u001b[39m concrete_type\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m script_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecursiveScriptModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpp_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_script.py:614\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;124;03mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m script_module \u001b[38;5;241m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> 614\u001b[0m \u001b[43minit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    618\u001b[0m RecursiveScriptModule\u001b[38;5;241m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    517\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m orig_value\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;66;03m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_concrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m cpp_module\u001b[38;5;241m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    523\u001b[0m script_module\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m scripted\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_recursive.py:542\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    539\u001b[0m     script_module\u001b[38;5;241m.\u001b[39m_concrete_type \u001b[38;5;241m=\u001b[39m concrete_type\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m script_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecursiveScriptModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpp_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_script.py:614\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;124;03mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    613\u001b[0m script_module \u001b[38;5;241m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> 614\u001b[0m \u001b[43minit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    618\u001b[0m RecursiveScriptModule\u001b[38;5;241m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_recursive.py:520\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    517\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m orig_value\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;66;03m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_concrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m cpp_module\u001b[38;5;241m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    523\u001b[0m script_module\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m scripted\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_recursive.py:546\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n\u001b[0;32m--> 546\u001b[0m     \u001b[43mcreate_methods_and_properties_from_stubs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_stubs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_stubs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;66;03m# Create hooks after methods to ensure no name collisions between hooks and methods.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;66;03m# If done before, hooks can overshadow methods that aren't exported.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_recursive.py:397\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    394\u001b[0m property_defs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mdef_ \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[1;32m    395\u001b[0m property_rcbs \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mresolution_callback \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m property_stubs]\n\u001b[0;32m--> 397\u001b[0m \u001b[43mconcrete_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_methods_and_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproperty_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperty_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_rcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_defaults\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \nArguments for call are not valid.\nThe following variants are available:\n  \n  aten::arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor:\n  Expected a value of type 'Optional[Device]' for argument 'device' but instead found type 'Tensor (inferred)'.\n  Inferred the value for argument 'device' to be of type 'Tensor' because it was not annotated with an explicit type.\n  \n  aten::arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor:\n  Argument end not provided.\n  \n  aten::arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor:\n  Argument end not provided.\n  \n  aten::arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!):\n  Argument end not provided.\n  \n  aten::arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!):\n  Argument out not provided.\n\nThe original call is:\n  File \"/root/ufront-test/torch_bert.py\", line 221\n    def forward(self, x, dtype, device):\n        return torch.arange(x, dtype=dtype, device=device)\n               ~~~~~~~~~~~~ <--- HERE\n"
     ]
    }
   ],
   "source": [
    "import iree.compiler as ireec\n",
    "from iree import runtime\n",
    "from torch_bert import BertModel, BertConfig\n",
    "import torch\n",
    "import time\n",
    "modelname = \"Bert\"\n",
    "input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "config = BertConfig(vocab_size_or_config_json_file=16000, hidden_size=768,\n",
    "    num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "model = BertModel(config=config)\n",
    "model.eval()\n",
    "\n",
    "ts_graph = torch.jit.script(model)\n",
    "module_ir = torch_mlir.compile(ts_graph, input_ids,\n",
    "                                    output_type=torch_mlir.OutputType.LINALG_ON_TENSORS)\n",
    "\n",
    "binary = compile_to_vmfb(module_ir, target_backend=\"llvm-cpu\")\n",
    "compiled_model = runtime.load_vm_flatbuffer(binary,backend=\"llvm-cpu\")\n",
    "        \n",
    "print(\"Performing benchmark...\")\n",
    "\n",
    "t1_stop = time.perf_counter()\n",
    "print(\"**** Model {} - Total Time: {:.3f}s\".format(modelname, t1_stop - t1_start)) # print performance indicator\n",
    "\n",
    "%timeit -n 100 compiled_model.forward(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6962246",
   "metadata": {},
   "source": [
    "#### RNN/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "179f4463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arg annotations should have one entry per function parameter (including self).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch_def import *\n",
    "modelname = \"LSTM\"\n",
    "batch_size = 8\n",
    "hidden_size = 128\n",
    "seq_size = 32\n",
    "input_size = 256\n",
    "try: \n",
    "    t1_start = time.perf_counter()\n",
    "\n",
    "    input = np.random.randn(batch_size, seq_size,hidden_size).astype(np.float32)\n",
    "    h0 = np.zeros((batch_size, hidden_size), dtype=np.float32)\n",
    "    c0 = np.zeros((batch_size, hidden_size), dtype=np.float32)\n",
    "\n",
    "    input, h0, c0 = torch.Tensor(input), torch.Tensor(h0), torch.Tensor(c0)\n",
    "    model = SimpleLSTM(input_size = 10, hidden_size = hidden_size, seq_size=seq_size)\n",
    "\n",
    "    model.train(mode=False)\n",
    "    ts_graph = torch.jit.script(model)\n",
    "    module_ir = torch_mlir.compile(ts_graph, input,\n",
    "                                        output_type=torch_mlir.OutputType.LINALG_ON_TENSORS)\n",
    "\n",
    "    binary = compile_to_vmfb(module_ir, target_backend=\"llvm-cpu\")\n",
    "    compiled_model = runtime.load_vm_flatbuffer(binary,backend=\"llvm-cpu\")\n",
    "    print(\"Performing benchmark...\")\n",
    "\n",
    "    t1_stop = time.perf_counter()\n",
    "    print(\"**** Model {} - Total Time: {:.3f}s\".format(modelname, t1_stop - t1_start)) # print performance indicator\n",
    "\n",
    "    %timeit -n 100 compiled_model.forward(input)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e0346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89bd1686",
   "metadata": {},
   "source": [
    "# UFront Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e877c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping ufront as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mProcessing ./ufront-0.1.1-cp310-cp310-manylinux_2_28_x86_64.whl\n",
      "Requirement already satisfied: onnx in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from ufront==0.1.1) (1.16.0)\n",
      "Requirement already satisfied: tf2onnx in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from ufront==0.1.1) (1.16.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from onnx->ufront==0.1.1) (1.26.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from onnx->ufront==0.1.1) (3.20.3)\n",
      "Requirement already satisfied: requests in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from tf2onnx->ufront==0.1.1) (2.31.0)\n",
      "Requirement already satisfied: six in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from tf2onnx->ufront==0.1.1) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from tf2onnx->ufront==0.1.1) (24.3.25)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from requests->tf2onnx->ufront==0.1.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from requests->tf2onnx->ufront==0.1.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from requests->tf2onnx->ufront==0.1.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/anaconda3/envs/triton/lib/python3.10/site-packages (from requests->tf2onnx->ufront==0.1.1) (2024.2.2)\n",
      "Installing collected packages: ufront\n",
      "Successfully installed ufront-0.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip uninstall ufront -y\n",
    "!pip install ./ufront-0.1.1-cp310-cp310-manylinux_2_28_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f876afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import time\n",
    "import torch\n",
    "from torchvision.models import resnet18, resnet50, squeezenet1_1, regnet_x_32gf, maxvit_t, shufflenet_v2_x1_5, inception_v3, mobilenet_v3_small, efficientnet_v2_s, densenet121, convnext_small\n",
    "import torchvision.models as models\n",
    "from ufront.pytorch.model import UFrontTorch\n",
    "import argparse\n",
    "import ctypes\n",
    "from iree.compiler import tools\n",
    "from iree import runtime\n",
    "import iree.runtime as ireert\n",
    "import iree.compiler as ireec\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import iree.runtime as ireert\n",
    "import iree.compiler as ireec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c90483",
   "metadata": {},
   "source": [
    "#### Vision models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ddbd21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3****Ufront->TOSA Time: 0.435s, TOSA->Binary Time: 7.299s, Total Time: 7.734s\n",
      "MobileNetV3 - 1.520 ± 0.014 ms\n",
      "ShuffleNetV2****Ufront->TOSA Time: 0.639s, TOSA->Binary Time: 5.097s, Total Time: 5.736s\n",
      "ShuffleNetV2 - 2.676 ± 0.008 ms\n",
      "ResNet18****Ufront->TOSA Time: 0.948s, TOSA->Binary Time: 4.569s, Total Time: 5.516s\n",
      "ResNet18 - 4.483 ± 0.006 ms\n",
      "ResNet50****Ufront->TOSA Time: 1.725s, TOSA->Binary Time: 8.104s, Total Time: 9.829s\n",
      "ResNet50 - 7.062 ± 0.006 ms\n",
      "SqueezeNet****Ufront->TOSA Time: 0.130s, TOSA->Binary Time: 3.031s, Total Time: 3.162s\n",
      "SqueezeNet - 1.085 ± 0.005 ms\n",
      "DenseNet121****Ufront->TOSA Time: 1.867s, TOSA->Binary Time: 13.395s, Total Time: 15.261s\n",
      "DenseNet121 - 12.030 ± 0.046 ms\n",
      "InceptionV3****Ufront->TOSA Time: 1.869s, TOSA->Binary Time: 10.554s, Total Time: 12.423s\n",
      "InceptionV3 - 15.420 ± 0.040 ms\n",
      "ViT_B16****Ufront->TOSA Time: 6.395s, TOSA->Binary Time: 10.204s, Total Time: 16.599s\n",
      "ViT_B16 - 12.510 ± 0.164 ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from benchmark import benchmark_module\n",
    "batch_size = 1\n",
    "input_sample = np.random.uniform(low=0.0, high=1.0, size=(batch_size, 3, 224, 224)).astype(np.float32)\n",
    "input = torch.Tensor(input_sample)\n",
    "\n",
    "model_list = {\"MobileNetV3\":mobilenet_v3_small(pretrained=False), \"ShuffleNetV2\":shufflenet_v2_x1_5(pretrained=False),\n",
    "            \"ResNet18\":resnet18(pretrained=False), \"ResNet50\":resnet50(pretrained=False), \"SqueezeNet\":squeezenet1_1(pretrained=False),\n",
    "            \"DenseNet121\":densenet121(pretrained=False), \"InceptionV3\":inception_v3(pretrained=False), \"ViT_B16\":models.vision_transformer.vit_b_16(weights=False, dropout=0.1)}\n",
    "\n",
    "for modelname, net in model_list.items():\n",
    "    net.train(False) \n",
    "\n",
    "    t1_start = time.perf_counter()\n",
    "    model = UFrontTorch(net, batch_size=batch_size, pass_weights=True) # convert torch model to ufront model\n",
    "    #This will trigger Rust frontend for actual model conversion and graph building\n",
    "    #operators can also be managed by python side (each operator here corresponding to an operator in the Rust computation graph)\n",
    "    output_tensors = model(inputs = [input])\n",
    "\n",
    "    #This will trigger model compilation, i.e., convert Rust computation graph to a unified high-level IR and lower it to TOSA IR\n",
    "    model.compile(optimizer={\"type\":\"sgd\", \"lr\":\"0.01\", \"momentum\":\"0\", \"nesterov\":\"False\", \"weight_decay\":\"0\"},\n",
    "                        loss='sparse_categorical_crossentropy', metrics=['accuracy', 'sparse_categorical_crossentropy'])\n",
    "    tosa_ir= model.dump_tosa_ir()\n",
    "\n",
    "    t1_stop = time.perf_counter()\n",
    "\n",
    "    binary = ireec.compile_str(tosa_ir,\n",
    "                    target_backends=[\"cuda\"], \n",
    "                    input_type=ireec.InputType.TOSA)\n",
    "    t2_stop = time.perf_counter()\n",
    "\n",
    "    print(modelname + \"****Ufront->TOSA Time: {:.3f}s, TOSA->Binary Time: {:.3f}s, Total Time: {:.3f}s\".format(t1_stop - t1_start, t2_stop - t1_stop, t2_stop - t1_start)) # print performance indicator\n",
    "    module = runtime.load_vm_flatbuffer(binary, driver=\"cuda\")\n",
    "    tms = []\n",
    "    for i in range(10):\n",
    "        ret = benchmark_module(module.vm_module, entry_function=\"forward\", inputs=[\"1x3x224x224xf32=1\"], device=\"cuda\")\n",
    "        tm = ret[0].time\n",
    "        tms.append(float(tm[0:-3]))\n",
    "    print(\"{} - {:.3f} ± {:.3f} ms\".format(modelname, np.mean(tms), np.std(tms)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cadb85c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "684ffd50",
   "metadata": {},
   "source": [
    "#### Language Model (Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "327f24a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling TOSA model...\n",
      "Compiling Binary...\n",
      "Bert****Ufront->TOSA Time: 6.852s, TOSA->Binary Time: 10.501s, Total Time: 17.353s\n",
      "7.21 ms ± 354 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "from ufront.pytorch.model import UFrontTorch \n",
    "import iree.compiler as ireec\n",
    "from iree import runtime\n",
    "from torch_bert import BertModel, BertConfig\n",
    "import torch\n",
    "import time\n",
    "input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "\n",
    "config = BertConfig(vocab_size_or_config_json_file=16000, hidden_size=768,\n",
    "    num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "net = BertModel(config=config)\n",
    "net.eval()\n",
    "\n",
    "t1_start = time.perf_counter()\n",
    "model = UFrontTorch(net, batch_size=1, pass_weights=True) # convert torch model to ufront model\n",
    "output_tensors = model(inputs = [input_ids, token_type_ids, input_mask])\n",
    "\n",
    "model.compile(optimizer={\"type\":\"sgd\", \"lr\":\"0.01\", \"momentum\":\"0\", \"nesterov\":\"False\", \"weight_decay\":\"0\"},\n",
    "                    loss='sparse_categorical_crossentropy', metrics=['accuracy', 'sparse_categorical_crossentropy'])\n",
    "\n",
    "print(\"Compiling TOSA model...\")\n",
    "tosa_ir= model.dump_tosa_ir()\n",
    "t1_stop = time.perf_counter()\n",
    "print(\"Compiling Binary...\")\n",
    "binary = ireec.compile_str(tosa_ir,\n",
    "                target_backends=[\"cuda\"], \n",
    "                input_type=ireec.InputType.TOSA)\n",
    "t2_stop = time.perf_counter()\n",
    "print(\"Bert****Ufront->TOSA Time: {:.3f}s, TOSA->Binary Time: {:.3f}s, Total Time: {:.3f}s\".format(t1_stop - t1_start, t2_stop - t1_stop, t2_stop - t1_start)) # print performance indicator\n",
    "module = runtime.load_vm_flatbuffer(binary, driver=\"cuda\")\n",
    "\n",
    "%timeit -n 100 module.forward(input_ids, token_type_ids, input_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aff4f3",
   "metadata": {},
   "source": [
    "#### RNN/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59bf17f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM****Ufront->TOSA Time: 0.138s, TOSA->Binary Time: 3.668s, Total Time: 3.806s\n",
      "2.03 ms ± 230 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ufront.pytorch.model import UFrontTorch\n",
    "import iree.compiler as ireec\n",
    "from iree import runtime\n",
    "import time\n",
    "import torch\n",
    "from torch_def import *\n",
    "import numpy as np\n",
    "batch_size = 8\n",
    "hidden_size = 128\n",
    "seq_size = 32\n",
    "input_size = 256\n",
    "input = np.random.randn(batch_size, seq_size,hidden_size).astype(np.float32)\n",
    "h0 = np.zeros((batch_size, hidden_size), dtype=np.float32)\n",
    "c0 = np.zeros((batch_size, hidden_size), dtype=np.float32)\n",
    "t1_start = time.perf_counter()\n",
    "input, h0, c0 = torch.Tensor(input), torch.Tensor(h0), torch.Tensor(c0)\n",
    "lstm = SimpleLSTM(input_size = 10, hidden_size = hidden_size, seq_size=seq_size)\n",
    "model = UFrontTorch(lstm, batch_size=batch_size, pass_weights=True)\n",
    "output_tensors = model(inputs = [input, h0, c0])\n",
    "\n",
    "model.compile(optimizer={\"type\":\"sgd\", \"lr\":\"0.01\", \"momentum\":\"0\", \"nesterov\":\"False\", \"weight_decay\":\"0\"},\n",
    "                      loss='sparse_categorical_crossentropy', metrics=['accuracy', 'sparse_categorical_crossentropy'])\n",
    "\n",
    "tosa_ir = model.dump_tosa_ir()\n",
    "t1_stop = time.perf_counter()\n",
    "\n",
    "binary = ireec.compile_str(tosa_ir,\n",
    "                target_backends=[\"cuda\"], \n",
    "                input_type=ireec.InputType.TOSA)\n",
    "t2_stop = time.perf_counter()\n",
    "\n",
    "print(\"LSTM****Ufront->TOSA Time: {:.3f}s, TOSA->Binary Time: {:.3f}s, Total Time: {:.3f}s\".format(t1_stop - t1_start, t2_stop - t1_stop, t2_stop - t1_start)) # print performance indicator\n",
    "\n",
    "module = runtime.load_vm_flatbuffer(binary, driver=\"cuda\")\n",
    "%timeit -n 100 module.forward(input, h0, c0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
